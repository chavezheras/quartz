<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üìù Daniel's Notes. on</title><link>https://garden.movingpixel.net/</link><description>Recent content in üìù Daniel's Notes. on</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><atom:link href="https://garden.movingpixel.net/index.xml" rel="self" type="application/rss+xml"/><item><title>Videogrep</title><link>https://garden.movingpixel.net/public_notes/Tools/Videogrep/</link><pubDate>Tue, 16 May 2023 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Tools/Videogrep/</guid><description>Videogrep is a command line tool that searches through dialog in video files and makes supercuts based on what it finds.</description></item><item><title>Zeeschuimer</title><link>https://garden.movingpixel.net/public_notes/Tools/Zeeschuimer/</link><pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Tools/Zeeschuimer/</guid><description>Zeeschuimer is a browser extension that monitors internet traffic while you are browsing a social media site, and collects data about the items you see in a platform&amp;rsquo;s web interface for later systematic analysis.</description></item><item><title>small large LLMs</title><link>https://garden.movingpixel.net/public_notes/small-large-LLMs/</link><pubDate>Fri, 05 May 2023 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/small-large-LLMs/</guid><description>My colleague Mercedes made me aware of this allegedly leaked document from a Google engineer, in which they make the case for open-sourcing their models.</description></item><item><title>Streaming Video Model</title><link>https://garden.movingpixel.net/public_notes/Tools/Streaming-Video-Model/</link><pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Tools/Streaming-Video-Model/</guid><description>Interesting video model that accounts for frames and sequences in a unified model. Abstract:
Video understanding tasks have traditionally been modeled by two separate architectures, specially tailored for two distinct tasks.</description></item><item><title/><link>https://garden.movingpixel.net/assets/images/tux.png/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/assets/images/tux.png/</guid><description/></item><item><title/><link>https://garden.movingpixel.net/moving_pixel/quartz/content/public_notes/Creanalytics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/moving_pixel/quartz/content/public_notes/Creanalytics/</guid><description/></item><item><title>Creanalytics</title><link>https://garden.movingpixel.net/public_notes/Creanalytics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Creanalytics/</guid><description>Creanalytics: a portmanteau of creative and analytic.
This idea came up when writing about the[[private/To do/Towards a Computational Video Essay | computational video essay]], largely in response to creative and analytic research methods coexisting under a unified computational framework.</description></item><item><title>Human-centered machine vision?</title><link>https://garden.movingpixel.net/public_notes/Human-centered-machine-vision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Human-centered-machine-vision/</guid><description>From the Click and Collect: Show Me Your Dataset event at Somerset House. See the twitter thread.
I presented ongoing research on [[public_notes/Creanalytics|Creanalytics]] forthcoming as an article in a special issue on Critical Technical Practice in the journal Convergence.</description></item><item><title>Mermaid diagrams</title><link>https://garden.movingpixel.net/public_notes/Tools/Mermaid-diagrams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Tools/Mermaid-diagrams/</guid><description>JavaScript based diagramming and charting tool that renders Markdown-inspired text definitions to create and modify diagrams dynamically.
It has a live editor for non-programmers, and has some tutorials.</description></item><item><title>Motion Canvas</title><link>https://garden.movingpixel.net/public_notes/Tools/Motion-Canvas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Tools/Motion-Canvas/</guid><description>Visualize Complex Ideas Programmatically. It&amp;rsquo;s a specialized tool designed to create informative vector animations and synchronize them with voice-overs.</description></item><item><title>NYU - Prague</title><link>https://garden.movingpixel.net/public_notes/Cultural-Metabolism-and-LLMs-on-wheels/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Cultural-Metabolism-and-LLMs-on-wheels/</guid><description>I was invited by the great people of the Digital Theory Lab at NYU to present this in Prague this summer.</description></item><item><title>The Digital Pastoral</title><link>https://garden.movingpixel.net/public_notes/The-Digital-Pastoral/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/The-Digital-Pastoral/</guid><description>I created the image below using #stablediffusion (before it was cool and every media scholar started doing it) and used it for my very short presentation at [[Transmediale 2023 | Transmediale 2023]].</description></item><item><title>Transmediale 2023</title><link>https://garden.movingpixel.net/public_notes/Transmediale-2023/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Transmediale-2023/</guid><description>I attended Transmediale and gave a very small presentation as part of the panel Toward Minor Tech.</description></item></channel></rss>