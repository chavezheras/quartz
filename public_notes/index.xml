<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Public_notes on</title><link>https://garden.movingpixel.net/public_notes/</link><description>Recent content in Public_notes on</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><atom:link href="https://garden.movingpixel.net/public_notes/index.xml" rel="self" type="application/rss+xml"/><item><title>Creative AI Theory and Practice</title><link>https://garden.movingpixel.net/public_notes/Creative-AI-Theory-and-Practice/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Creative-AI-Theory-and-Practice/</guid><description>Twitter thread about this from King&amp;rsquo;s AI institute
The recording of this event is online.</description></item><item><title>Human-centered machine vision?</title><link>https://garden.movingpixel.net/public_notes/Human-centered-machine-vision/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Human-centered-machine-vision/</guid><description>From the Click and Collect: Show Me Your Dataset event at Somerset House. See the twitter thread. I presented ongoing research on the [[private/To do/Towards a Computational Video Essay | Computational Supercut]], forthcoming as an article in a special issue on Critical Technical Practice in the journal Convergence.</description></item><item><title>Mermaid diagrams</title><link>https://garden.movingpixel.net/public_notes/Tools/Mermaid-diagrams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Tools/Mermaid-diagrams/</guid><description>JavaScript based diagramming and charting tool that renders Markdown-inspired text definitions to create and modify diagrams dynamically.
It has a live editor for non-programmers, and has some tutorials.</description></item><item><title>Motion Canvas</title><link>https://garden.movingpixel.net/public_notes/Tools/Motion-Canvas/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Tools/Motion-Canvas/</guid><description>Visualize Complex Ideas Programmatically. It&amp;rsquo;s a specialized tool designed to create informative vector animations and synchronize them with voice-overs.</description></item><item><title>NYU - Prague</title><link>https://garden.movingpixel.net/public_notes/Digital-Theory-Lab-NYU-Prague/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Digital-Theory-Lab-NYU-Prague/</guid><description>Cultural Metabolism and LLMs on wheels Abstract Large Language Models ( #LLMs ) trained on vast amounts of internet text are able to simulate natural language structures to a degree that enables novel interfaces for human-computer interaction.</description></item><item><title>The Digital Pastoral</title><link>https://garden.movingpixel.net/public_notes/The-Digital-Pastoral/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/The-Digital-Pastoral/</guid><description>I created the image below using #stablediffusion for my very short presentation at [[Transmediale 2023 | Transmediale 2023]].
![[assets/minor_tech_DCH.jpg]]</description></item><item><title>Transmediale 2023</title><link>https://garden.movingpixel.net/public_notes/Transmediale-2023/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://garden.movingpixel.net/public_notes/Transmediale-2023/</guid><description>I attended Transmediale and gave a very small presentation as part of the panel Toward Minor Tech.</description></item></channel></rss>