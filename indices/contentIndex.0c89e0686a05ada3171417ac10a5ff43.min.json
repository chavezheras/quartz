{"/":{"title":"üìù Daniel's Notes.","content":"\nWelcome to my public notes. Not quite a blog, but not a timeline either, this kind of personal wiki of linked notes is an attempt to gradually move away from social media platforms but still keep some kind of connection with the public internet.\n\nIt is very close to the idea of a digital garden, in that it takes inspiration from the early web; from [hypertext](http://www.eastgate.com/garden/Enter.html) and the [Memex](https://en.wikipedia.org/wiki/Memex), and and from owning the files one shares. In keeping with these ideas, the notes in this garden are never fully finished and are meant to grow (or wither) organically.\n\nYou can read [A Brief History \u0026 Ethos of the Digital Garden](https://maggieappleton.com/garden-history), and [The Garden and the Stream](https://hapgood.us/2015/10/17/the-garden-and-the-stream-a-technopastoral/) to find out more about digital gardens.\n\n---\n\n","lastmodified":"2023-07-14T20:59:07.404003892Z","tags":null},"/.ipynb_checkpoints/_index-checkpoint":{"title":"üìù Daniel's Notes.","content":"\nWelcome to my public notes. Not quite a blog, but not a timeline either, this kind of personal wiki of linked notes is an attempt to gradually move away from social media platforms but still keep some kind of connection with the public internet. It is very close to the idea of a digital garden, in that it takes inspiration from the early web; from hypertext and the memex, and and from owning the files one shares. In keeping with these ideas, the notes in this garden are never fully finished and are meant to grow (or wither) organically. Read A Brief History \u0026 Ethos of the Digital Garden, and The Garden and the Stream to know more about digital gardens.\n","lastmodified":"2023-07-14T20:59:07.404003892Z","tags":null},"/public_notes/5-principles-of-life":{"title":"5 principles of life","content":"---\n\nSome very high-level background about cell-biology for my talk on  [[public_notes/Cultural Metabolism and LLMs on wheels|Cultural Metabolism and LLMs on wheels]] at Prague earlier this year.\n\nAccording to Paul Nurse:\n\n1. Living things are bounded physical entities\n2. The bounded entity is the chemical and informational machine\n3. This chemical and informational machine has a hereditary system\n\t1. The system prefigures how the entity works\n\t2. But has in-built variability to respond to its environment ([epigenetics](https://en.wikipedia.org/wiki/Epigenetics))\n4. The entity can therefore evolve by natural selection\n5. The living thing can have purpose of being better adapted to the life state it finds itself\n\nFrom his book [What is Life?](https://www.waterstones.com/book/what-is-life/paul-nurse/9781788451406)\n\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Creanalytics":{"title":"Creanalytics","content":"---\n\n_Creanalytics_: a portmanteau of creative and analytic.\n\nThis idea came up when writing about the[[private/To do/Towards a Computational Video Essay | computational video essay]], largely in response to creative and analytic research methods coexisting under a unified computational framework.   In the case of deep learning, this could be seen as _generation_ through _prediction_, similar to the [analytic-synthetic distinction](https://www.williamcotton.com/articles/chatgpt-and-the-analytic-synthetic-distinction) which I found in this piece about [analytic augmentation in prompt engineering](https://github.com/williamcotton/empirical-philosophy/blob/main/articles/from-prompt-alchemy-to-prompt-engineering-an-introduction-to-analytic-agumentation.md).\n\nUpdate: this is now an article! Forthcoming in a special Issue on critical technical practice in the journal [Convergence: The International Journal of Research into New Media Technologies](https://journals.sagepub.com/home/con) \n\n![[assets/images/Screenshot from 2023-07-02 18-08-10.png]]\n\nAlso related to a project in collaboration with my wonderful colleagues [Jonathan Gray](https://jonathangray.org/) and [Mercedes Bunz](https://www.kcl.ac.uk/people/mercedes-bunz) on using video editing techniques as data analysis. We commissioned [Sam Lavigne](https://lav.io/) to work on  his [[public_notes/Tools/Videogrep|Videogrep]] tool! More on this soon!","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Creative-AI-Theory-and-Practice":{"title":"Creative AI Theory and Practice","content":"---\n\nThe recording of our [symposium on Creative AI](https://www.kcl.ac.uk/events/creative-ai-theory-and-practice) is now available. Presentations are indexed at different timestamps.\n\n\u003eOn Friday 27 January 2023, Creative AI Lab at King‚Äôs College London/Serpentine (Professor Mercedes Bunz and curator Eva J√§ger as Lab‚Äôs co-founders, Dr Daniel Ch√°vez Heras, PhD student Alasdair Milne, Professor Joanna Zylinska) hosted a one-day symposium supported by the King‚Äôs Institute for Artificial Intelligence.\n\n\n\u003ciframe title=\"Creative AI: Theory and Practice\" src=\"https://www.youtube.com/embed/xuZsf3ZX7k8?feature=oembed\" height=\"113\" width=\"200\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n\n\n\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Cultural-Metabolism-and-LLMs-on-wheels":{"title":"NYU - Prague","content":"---\n\nI was invited by the great people of the [Digital Theory Lab at NYU](https://digitalhumanities.nyu.edu/projects/digital-theory-lab/) to present this in Prague this summer. Very much looking forward to it!\n\n# Cultural Metabolism and LLMs on wheels\n## Abstract\nLarge Language Models ( #LLMs ) trained on vast amounts of internet text are able to simulate natural language structures to a degree that enables novel interfaces for human-computer interaction. One key function in these systems can be observed at the micro level, when the next token in an unfolding series is predicted and concatenated recursively, in other words, at the logical step when calculation becomes synthesis.\nIn this presentation I focus on this very localised but crucial moment of exchange, drawing a parallel with metabolic processes in cellular biology, to explore the notion of self-contained and embedded autonomous systems: LLMs+ or LLMs on wheels.\n\nThis is related to my interest in agent-based modelling and complex systems in cultural domains. Inspired among other influences by [Thilo Gros](https://reallygross.de/me) and his work at [Biond Lab](https://biond.org/)\nI love his [complexity papers videos](https://www.youtube.com/@complexitypapers)!\n\n\n\u003ciframe title=\"Estimation of functional diversity and species traits from ecological monitoring data\" src=\"https://www.youtube.com/embed/BsNkfj0HZBs?feature=oembed\" height=\"113\" width=\"200\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Human-centered-machine-vision":{"title":"Human-centered machine vision?","content":"---\n\nFrom the _Click and Collect: Show Me Your Dataset_ event at Somerset House. I was invited as part of a panel with [Charlotte Webb](https://www.feministinternet.com/) and [Kristina Pulejkova](https://kristinapulejkova.com/) to discuss human-centred design in relation to AI. \n\nI presented ongoing research on [[public_notes/Creanalytics|Creanalytics]] forthcoming as an article in a special issue on _Critical Technical Practice_ in the journal [Convergence](https://journals.sagepub.com/home/con):\n\n\u003ciframe title=\"Daniel Ch\u0026amp;aacute;vez Heras: Artist Presentation\" height=\"240\" width=\"426\" src=\"https://player.vimeo.com/video/806299101?h=31224a9af9\u0026amp;app_id=122963\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.775 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n---\n\nThe recording of the panel is also now available here starting at about 6:14\n\n\u003ciframe title=\"vimeo-player\" src=\"https://player.vimeo.com/video/806397141?h=0470a6a1ef\" #t=22s?autoplay=0 width=\"640\" height=\"360\" frameborder=\"0\"    allowfullscreen\u003e\u003c/iframe\u003e\n\n--- \n\nSee the  [twitter thread](https://twitter.com/sh_studios_/status/1626900453828005889?s=20) for more on the larger event.\n\n![[assets/FpPpVlVWIAE1uFq.jpeg]]","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/The-Digital-Pastoral":{"title":"The Digital Pastoral","content":"---\n\nI created the image below using #stablediffusion (before it was cool and every media scholar started doing it) and used it for my very short presentation at [[Transmediale 2023 | Transmediale 2023]].\n\n\n![[assets/minor_tech_DCH.jpg]]\n\nI can't recall the exact prompt, but I do remember I referenced the illustrations on [Jakub R√≥≈ºalski](https://jrozalski.com/) whom I discovered through [Scythe](\u003chttps://en.wikipedia.org/wiki/Scythe_(board_game)\u003e).\n\nAnd here's [a link to the newspaper](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf)!","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Tools/Mermaid-diagrams":{"title":"Mermaid diagrams","content":"\n\u003eJavaScript based diagramming and charting tool that renders Markdown-inspired text definitions to create and modify diagrams dynamically.\n\nIt has a [live editor](https://mermaid.live/) for non-programmers, and has [some tutorials](https://mermaid.js.org/config/Tutorials.html). Maybe relevant for students. There's a Jupyter integration too. Interesting for #criticaltechnicalpractice \nMaybe I can use this to render an maintain a web version of the [[private/Projects/Large/VANGE]] diagram in my office.\n\n![Mermaid](https://mermaid.js.org/header.png)\n\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Tools/Motion-Canvas":{"title":"Motion Canvas","content":"---\n\n\u003eVisualize Complex Ideas Programmatically.\n\u003eIt's a specialized tool designed to create informative vector animations and synchronize them with voice-overs. It's not meant to be a replacement for traditional video editing software.\n\nProcedural animation tool, that can be integrated with a web-editor to sync sound and produce animated videos. Great for education, documentation, and reflection in #criticaltechnicalpractice Reminds me a bit of Flash action script, but #FOSS and nicer for rapid development and to comunicate abstract ideas precisely. Think of [Freya Holm√©r](https://www.youtube.com/@Acegikmo) videos or [Ms Coffee Beans](https://www.youtube.com/@AICoffeeBreak).\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/H5GETOP7ivs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Tools/Streaming-Video-Model":{"title":"Streaming Video Model","content":"---\n\n[Interesting video model](https://arxiv.org/abs/2303.17228#) that accounts for frames and sequences in a unified model. \nAbstract:\n[](https://arxiv.org/search/cs?searchtype=author\u0026query=Zha%2C+Z)\n\n\u003e Video understanding tasks have traditionally been modeled by two separate architectures, specially tailored for two distinct tasks. Sequence-based video tasks, such as action recognition, use a video backbone to directly extract spatiotemporal features, while frame-based video tasks, such as multiple object tracking (MOT), rely on single fixed-image backbone to extract spatial features. In contrast, we propose to unify video understanding tasks into one novel streaming video architecture, referred to as Streaming Vision Transformer (S-ViT). S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks. Then the frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks. The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. We believe that the concept of streaming video model and the implementation of S-ViT are solid steps towards a unified deep learning architecture for video understanding. Code will be available at [this https URL](https://github.com/yuzhms/Streaming-Video-Model).\n\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Tools/Videogrep":{"title":"Videogrep","content":"---\n\n\u003eVideogrep is a command line tool that searches through dialog in video files and makes [supercuts](https://vimeo.com/440746435) based on what it finds. It will recognize .srt or .vtt subtitle tracks, or transcriptions that can be generated with vosk, pocketsphinx, and other tools.\n\nSam posted an implementation of Videogrep that runs on Colab, which made me think of running this on a Jupyter Lab instance from our College's computer cluster, and then possibly using it for teaching. The college does not host its own notebook instance, as far as I know, but the cluster can serve a headless Jupyter that can be \"tunnelled in\" through ssh.\n\nHere's a supercut of [recent lecture](https://www.youtube.com/watch?v=b6ogLgWnpes) by  [Shannon Mattern](https://cinemastudies.sas.upenn.edu/people/shannon-mattern) at KCL called \"Modeling Doubt, Coding Humility: A Speculative Syllabus\"\n\n\u003ciframe title=\"Doubt ‚Äï with Shannon Mattern\" height=\"240\" width=\"426\" src=\"https://player.vimeo.com/video/830237949?h=5fb9301c4d\u0026amp;app_id=122963\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.775 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n\nInterestingly, not one single mention of \"code\" or \"coding\", nor of \"humility\". I recommend watching the whole talk, though.\n\nHere's the [repository](https://github.com/antiboredom/videogrep)for Videogrep. Here's the [Colab minimal example](https://t.co/QGKTLxOZ52).\nAnd here are other examples from Sam's repo:\n\n-   [silence extraction](https://github.com/antiboredom/videogrep/blob/master/examples/only_silence.py)\n-   [automatically creating supercuts](https://github.com/antiboredom/videogrep/blob/master/examples/auto_supercut.py)\n-   [creating supercuts based on youtube searches](https://github.com/antiboredom/videogrep/blob/master/examples/auto_youtube.py)\n-   [creating supercuts from specific parts of speech](https://github.com/antiboredom/videogrep/blob/master/examples/parts_of_speech.py)\n-   [creating supercuts from spacy pattern matching](https://github.com/antiboredom/videogrep/blob/master/examples/pattern_matcher.py)","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Tools/Zeeschuimer":{"title":"Zeeschuimer","content":"---\n\n\u003e Zeeschuimer is a browser extension that monitors internet traffic while you are browsing a social media site, and collects data about the items you see in a platform's web interface for later systematic analysis. Its target audience is researchers who wish to systematically study content on social media platforms that resist conventional scraping or API-based data collection.\n\n\n![Zeeschuimer](https://github.com/digitalmethodsinitiative/zeeschuimer/blob/master/images/example_screenshot.png?raw=true)\n\nThis would definitely be of interest to some of my UX students as a way to integrate in their usability tests and cultural probes when analysing social media usage.\nRepository [here](https://github.com/digitalmethodsinitiative/zeeschuimer).\n\n\n\n\n\n\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Transmediale-2023":{"title":"Transmediale 2023","content":"---\n\nI attended [Transmediale](https://2023.transmediale.de/en) and gave a very small presentation as part of the panel *Toward Minor Tech*.\n\n[![Towards a Minor Tech Newspaper](assets/images/minor_tech_news.jpeg)](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf)\n\nThis is the result of a workshop organised by by Geoff Cox and Christian Ulrik Andersen every year as part of [SHAPE Digital Citizenship](https://shape.au.dk/en/) \u0026 [Digital Aesthetics Research Center, Aarhus University](https://darc.au.dk/) and [Centre for the Study of the Networked Image, London South Bank University](https://www.centreforthestudyof.net/), this year in collaboration with [King‚Äôs College London](https://www.kcl.ac.uk/). \n\nThe workshop took place over several days in London and inlcuded public talks from Marloes de Valk on the [damaged earth catalog](https://damaged.bleu255.com/) and from Tung-Hui Hu on [digital lethargy](https://thephotographersgallery.org.uk/whats-on/talk-digital-lethargy).\nMy contribution to the publication was titled: *A minor critique of minor tech* and can be found as a PDF, along with all other contirbutions, by clicking [here](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf).\n\n![minor tech panel](assets/images/minor_tech_panel.jpeg)\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Video-as-data":{"title":"Video as data in the US","content":"---\n\nThis looks interesting!\nSee twitter post [here](https://twitter.com/Kaiping_Chen/status/1660703434155106313).\n\n![[assets/images/Pasted image 20230523190034.png]]","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/Video-understanding-community":{"title":"Video-understanding community","content":"---\n\nSome interesting recent work on video understanding tasks and practitioners from computer science. \n\n[CMD Challenge](https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/challenge.html)\nFrom the [VGG group at Oxford](https://www.robots.ox.ac.uk/~vgg/). It uses a version of the [condensed movie dataset](https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/)challenge, based on the same Movieclip collection I describe in [[public_notes/Creanalytics|Creanalytics]]. See [their repo](https://github.com/m-bain/CondensedMovies-chall) for challenge details, and the description below:\n\n\u003e__Focus__: The focus of this challenge is on the long-range understanding of high-level narrative structure in movies.\n\n\u003e__Overview__: In the challenge, participants are invited to build a system to retrieve 2-3 minute video clips from movies using corresponding high-level natural language descriptions and a wide range of pre-computed visual features from several pre-trained expert models. Each 2-3 minute clip constitutes a key scene from a movie, each representing important parts in the storyline. Each clip is accompanied by a high-level semantic description which describes the storyline. This includes the motivations of the characters, actions, scenes, objects, interactions and relationships. Participants will use a new challenge version of the Condensed Movies Dataset (CMD) for both training and testing of their retrieval systems. \n\nThis group/challenge is related to the [[public_notes/Video as data|Video as data]] ICA workshop. And also to a broader push to integrate natural language and visual understandings. See for example the work of [Max Bain](https://www.maxbain.com/), [Lisa Anne Hendricks](https://lisaanne.github.io/) at Deepmind and [Andrew Brown](https://www.robots.ox.ac.uk/~abrown/), ex VGG now at Meta. See below:\n\nhttps://www.youtube.com/watch?v=GzIphByhXDc\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null},"/public_notes/small-large-LLMs":{"title":"small large LLMs","content":"---\nMy colleague [Mercedes Bunz](https://mercedesbunz.net/) made me aware of this allegedly [leaked document](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) from a Google engineer, in which they make the case for open-sourcing their models. TLDR; The argument is that owning and cultivating the ecosystem for innovation is more valuable than keeping the models fenced off.\n\nCaveats notwithstanding, say for instance that the diminishing value of training does not account for people's salaries in publicly-funded institutions, it is still an interesting read, especially the timeline narrating all the developments. Good for teaching, but also to make sense of the various recent moves in the field.\n\nSome of the models and mentioned:\n[LLaMA](https://arxiv.org/pdf/2303.16199.pdf) ‚Äï Meta\n[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)‚Äï Stanford\n[Alpaca LoRA](https://github.com/tloen/alpaca-lora)‚Äï Stanford + Eric Wang. See [paper here](https://arxiv.org/pdf/2106.09685.pdf).\n[A Chatbot interface for Alpaca](https://github.com/deep-diver/LLM-As-Chatbot)\n[Dolly 15k instructions dataset](https://huggingface.co/datasets/c-s-ale/dolly-15k-instruction-alpaca-format)\n\u003e `databricks-dolly-15k` is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language models to exhibit the magical interactivity of ChatGPT.\n\n[GPT4all](https://github.com/nomic-ai/gpt4all)‚Äï open stack pipeline based on JPT-J and LLaMA\n[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)\n\u003ean open-source Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT\n\nThis las one was developed by a student-led university consortium called [LMSYS Org](https://lmsys.org/about/)!\n\n","lastmodified":"2023-07-14T20:59:07.548004483Z","tags":null}}