{"/":{"title":"üìù Daniel's Notes.","content":"\n\n","lastmodified":"2023-07-02T17:28:23.545113964Z","tags":null},"/.ipynb_checkpoints/_index-checkpoint":{"title":"üìù Daniel's Notes.","content":"\n\n","lastmodified":"2023-07-02T17:28:23.545113964Z","tags":null},"/private/Projects/Large/AI-opera":{"title":"AI opera","content":"---\n\nI've been going to the opera recently, and it is really great. As i sit there in the dark I think about the type of opera I'd make, and it involves a story told in time jumps between the future and the recent past of AI, and quite possibly about conciousness.\nFor one act or set piece I imagine it has a giant screen arrangement suspended from the ceiling, a bit like the [[private/Transmediale notes#Unconscionable Maps|the works of Evan Roth]] and a lonley hacker \"plugged in\" through a funnel of cables.\nThematically (and musically) realted to [Deus Ex](https://en.wikipedia.org/wiki/Deus_Ex_(video_game)), as one part is set in post hydro-wars carbon-wars of the near future.\nOther locations inlcude a slum near a pumping station in south east asia like [Lower Bicutan](https://goo.gl/maps/kHa1FhNfRT3f2d3u5)\nI need to watch more contemporary operas, and find whatever happened to that lady that worked at the royal opera house whom I met at Sussex.\n\nChoreographed forearm performance can be used to convey inductive computing. See: [Choreographed, limb-centric performance / Sadeck Waff](https://youtu.be/ekibVX3gK5Q)\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Projects/Large/Art-for-Public-AI-report":{"title":"Creative AI report","content":"---\n\nSee [[private/Projects/Things I think we should do#Projects]] for more:\n\n![[private/Projects/Things I think we should do#Projects]]\n\nThis would need external funding, perhaps from a smaller start to scope it, e.g.\n\n¬£1.5-2k to scope it --\u003e departamental grant\n¬£10k to do a first edition --\u003e Turing/TAS money\n\nHad a first meeting about this with Eva to do this as part of their Future of Art Ecosystems series.\nSuggested to bring in [Tom Flemming](https://www.tfconsultancy.co.uk/) as a consultant / delivery partner.\nTwo very relevant resources on [gray literature](https://en.wikipedia.org/wiki/Grey_literature) and the creative economy:\nPolicy and Evidence Centre (PEC) from NESTA (they have reports such as [this](https://cdn2.assets-servd.host/creative-pec/production/assets/publications/PEC-and-Nesta-report-The-art-in-the-artificial.pdf))\nAnd did a recent event on policy:\n\n[https://www.youtube.com/live/ESOiJ0Xlszo?feature=share](https://www.youtube.com/watch?v=ESOiJ0Xlszo)\n\nAnd [this one](https://openuk.uk/wp-content/uploads/2021/07/State-of-Open-Phase-Two.pdf), from [Open UK](https://openuk.uk/) (\"the UK organisation for the business of Open Technology\")\n\nPapers mentioned in the workshop on Public AI held at KCL on 12 May 2023:\n\n- [ ] [Democratising AI: Multiple Meanings, Goals, and Methods](https://arxiv.org/abs/2303.12642)\n- [ ] [Against ‚ÄúDemocratizing AI‚Äù](https://link.springer.com/article/10.1007/s00146-021-01357-z)\n\n\n\n\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Projects/Large/Promptology":{"title":"Promptology","content":"---\n\nA coffee table book about vernacular AI art as it developed before/besides [DALLE2](https://openai.com/dall-e-2/) and subsequent corporate takeover.\n\nThe idea is to have several large super striking images and their prompts printed in large format and high quality, accompanied by essays on text-2-image systems. I've been thinking about whether these prompts might be gotten from a specific group of people, or with a specific theme, but I now think a broader theme like possible futures might be better.\nAnd also possibly something we can get from  [[private/Projects/Things I think we should do| a \"prompetition\" organised by CAIL]]  \nWe ask people to send us their prompts and we select, render, and upscale them using CREATE. We offer some form of prize.\nI also thought about workshoping these images between artists and essay writers.","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Projects/Large/Synthetic-Media-and-Generative-Approaches-in-the-Humanities":{"title":"Synthetic Media and Generative Approaches in the Humanities","content":"---\n\nLarger project. Work on bid for this  üí∞\n\nReference for this event and ask Aurora about recordings:\n\n![Responses from media studies towards a \"new paradigm\" of image production](https://pbs.twimg.com/media/Fm7WJcUXgAA5V9F?format=jpg\u0026name=4096x4096)\n\n![[assets/FpqZ7QIXgAQdSuD.jpeg]] This image was created using #controlnet and #stablediffusion ","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Projects/Large/VANGE":{"title":"VANGE","content":"---\n\nVideo Analysis Generative Engine\n\nAn idea for a tool to make computational video essays. There is a diagram for this in my office.\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Projects/Small/Computational-video-essay":{"title":"Computational video essay","content":"---\n\nUpdate the dataset: mine the [Movieclips YT channel](https://www.youtube.com/@MOVIECLIPS) again\nTake control over the database?\nAnnotate the dataset with object detection.\nRefactor code to have a front end, downloading the YT videos on the fly.\nPublish the dataset and add a data paper.\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Projects/Things-I-think-we-should-do":{"title":"CAIL: Things I think we should do","content":"---\n\nGive substance to some of the weaker claims of the position paper.\n\n## Education\nSince we (at KCL) are actually a large teaching operation, and CAIL now has critical mass of permanent members of staff, we should consider specific training/education initiatives that fit our activities. Possible options:\n- Creative AI module ‚Äï level 6, practice-based, in the flex curriculum, using [e-research](https://docs.er.kcl.ac.uk/CREATE/access/)/NMEs teaching support. Possibly with invited lecturers each week? Could this be interesting to artists (Serpentine)?\n- Summer workshop ‚Äï once a year, sprint-like, theme-based, artists (Serpentine), academics, students, and engineers.\n- Student curating ‚Äï students find and curate AI Art advised by CAIL\n- Creative AI competition\n\n## Projects\n- Book sprint with Meson Press? ‚ÄïMercedes\n- [[private/Projects/Large/Art for Public AI report|Creative AI report]] Survey of the field leveraging research expertise form colleagues in the department, and Serpentine could broker access to the artists for interviews, etc. Maybe can be updated every year? ref. [Oxford report](https://www.oii.ox.ac.uk/news-events/reports/ai-the-arts/). \u003c--are we not much better positioned to do this type of thing?\n\n## Research outputs\n(sorry for the corporate temrinology)\n\n-  [ ] ~~JVC edited volume~~\n- Events\n\t-  [x] CAIL co-sponsored [[Transmediale 2023|Toward a Minor Tech]]\n\t- [x] CAIL position paper Launch event\n\t-  [ ] Book launch / conversation ‚ÄïJussi's new book, others?","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Teaching/7AAVDM52":{"title":"7AAVDM52","content":"---\n\n- [x] Share this example of [successful Hollywood movies](https://informationisbeautiful.net/visualizations/what-is-the-most-successful-hollywood-movie-of-all-time/#interactive) for my visualisation students. Last lecture of the semester!\n- [x] Update KEATS with weekly readings\n- [x] Upload visualisation types cheat sheet and article.\n- [x] Update about assessment in KEATS\n- [x] Note to seminar leaders\n\nPossible guest lecturer: [Laura Koesten](https://laurakoesten.github.io)\n\n\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Teaching/7AAVDM57":{"title":"7AAVDM57","content":"---\n\nIdeas for students projects:\n\n- [x] Use scenarios/diagrams form the Scientific Trieste's [[private/Transmediale notes#Unconscionable Maps|Scientific Dreaming]] project.\n\n- [x] Use delivery workers a one of the target groups for students.\n\n\n\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/Teaching/Marking":{"title":"Marking","content":"---\n\n- [x] Second mark 6AAVC403 Digital Gaming deadline 28 February\n- [x] First marking User-Centered Research 7AAVDM58 8 March\n- [x] Second mark 5AAVC209 Design in the Digital World ASAP!\n\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/To-do/Cinema-and-Machine-Vision":{"title":"Cinema and Machine Vision","content":"---\n\n- [x] Finish this book!\n\n\n\n\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/To-do/Human-centered-machine-vision":{"title":"Human'centered machine vision?","content":"\n---\n\n\nUpload the slides fro this event, and a note about the event with the recording, pictures, etc.","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/To-do/Reading-List":{"title":"Reading List","content":"---\n\n- [ ] [Field-level differences in paper and author characteristics across all fields of science in Web of Science, 2000‚Äì2020](https://doi.org/10.1162/qss_a_00246)\nI want to know from this how digital humanities fare compared to traditional arts and humanities\n\n\u003eAbstract\n\nWith increasing availability of near-complete, structured bibliographical data, the past decade has seen a rise in large-scale bibliometric studies attempting to find universal truths about the scientific communication system. However, in the search for universality, fundamental differences in knowledge production modes and the consequences for bibliometric assessment are sometimes overlooked. This article provides an overview of article and author characteristics at the level of the OECD minor and major fields of science classifications. The analysis relies on data from the full Web of Science in the period 2000‚Äì2020. The characteristics include document type, median reference age, reference list length, database coverage, article length, co-authorship, author sequence ordering, author gender, seniority, and productivity. The article reports a descriptive overview of these characteristics combined with a principal component analysis of the variance across fields. The results show that some clusters of fields allow inter-field comparisons, and assumptions about the importance of author sequence ordering, while other fields do not. The analysis shows that major OECD groups do not reflect bibliometrically relevant field differences, and that a reclustering offers a better grouping.\n\n\n\n- [ ] [Deepfakes and the Epistemic Apocalypse](https://philpapers.org/archive/HABDAT-2.pdf)\n\n\u003eAbstract:\n\u003eIt is widely thought that deepfake videos are a significant and unprecedented threat to\nour epistemic practices. In some writing about deepfakes, manipulated videos appear as the\nharbingers of an unprecedented\nepistemic apocalypse. In this paper I want to take a critical\nlook at some of the more catastrophic predictions about deepfake videos. I will argue for\nthree claims: 1) that once we recognise the role of social norms in the epistemology of\nrecordings, deepfakes are much less concerning, 2) that the history of photographic\nmanipulation reveals some important precedents for deepfakes which correct claims about\nthe novelty of deepfakes, and 3) that proposed solutions to deepfakes have been overly\nfocused on technological interventions. My overall goal is not so much to argue that\ndeepfakes are not a problem, but to argue that behind concerns around deepfakes lie a\nmore general class of social problems about the organisation of our social practices.\n\n- [ ] [Deceitful Media](https://global.oup.com/academic/product/deceitful-media-9780190080365?cc=nl\u0026lang=en\u0026) by Simone Natale\n- [ ] [A pattern language for generative AI](https://crfm.stanford.edu/2023/03/13/alpaca.html)\n- [ ] ","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/To-do/To-buy":{"title":"To buy","content":"---\n\n- [x] Coat hanger\n- [x] Floor divider\n- [x] Bin bags\n- [x] Milk\n- [x] Hula Hops for lunch\n- [x] NAS\n- [x] Active sitting chair\n- [ ] Plane tickets to MX\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/To-do/To-do-weekend":{"title":"To do weekend","content":"---\n\n- [x] Closet door / finish closet\n- [x] Storage wars en buhardilla\n- [x] Weather station batteries\n- [x] Install batteries\n- [ ] Credit card\n- [x] Pay hot water Wandsworth\n- [x] Buy Kurious tickets\n- [ ] Meal plan\n- [x] Buy peak district train tickets","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/private/To-do/Towards-a-Computational-Video-Essay":{"title":"Towards a Computational Video Essay","content":"---\n\nLatest revisions:\n\n- [x] Discuss the main line of argumentation more explicitly at the beginning of the article (to make the contributions more visible).\n- [x] Make an effort to establish a better dialogue with the digital humanities/cultural analytics (e.g. via Arnold and Tilton's work) in the theoretical framework.\n- [x] In practice these two steps mostly involve bringing up a few sentences/aspects here and there.\n- [x] Divide preliminary results (perhaps call initial insights and reflection) and conclusion.\n\n\n\nReviewer(s)' Comments to Author:  \n  \nReviewer: 1  \n  \nComments to the Author  \nI can see that the manuscript has been revised, especially in the introduction. To me, it is still necessary to engage more extensively and more clearly with the literature to support the paper's claim of relevance and originality. The theoretical framework in the revised manuscript is not very elaborate, and the author also acknowledges it is as such (‚Äúnot to be understood as a fully-fledged theoretical framework‚Äù and ‚ÄúSave for notable exceptions [‚Ä¶] it would be fair to say‚Ä¶‚Äù). It is necessary to position your own research more clearly within the field.  \n\nPlease look into the following:  \n- [x] include a more substantial literature review on the fields to which you contribute. It is necessary to elaborate on the ideas of the main authors of each field as it enables to clarify your own contribution  \n\n~~not sure if elaborating on the main authors mentioned, but can write something about distant viewing, explain how it applies the principles of distant reading to moving images, and how it is concerned with finding patterns in large collections of cultural artefacts. \nMy contribution aims to make visible the sampling operation and data modelling involved in creating the conditions for these distance, namely, how distant viewing spatialises time-based media. \nThe practice I propose re-inscribes these patterns found through computational analysis in time, returning the critic to familiar mode of analysis by giving data objects duration\nIs not so much distant viewing, but rather condensed viewing; a computationally-enabled form of probing moving images in time, through the familiar strategies of montage and the syntactic and synoptic apparatus of (dis)continuity as a meaning-making modality that operates through sequential rather than spatial proximity. \nThus not plotting moving images as data points in space, but making these data points flow in a timeline. Using the critic's ability to think through time, and to give these proximities a narrative content.~~\n\n~~up close by sampling~~\n\n- [x] be more clear and more specific about your own contribution to all the fields, especially to the field of video essays and to the field of digital humanities  \n- [x] integrate a clearer dialogue between existing scholarship/findings and own method in the sections where you introduce and evaluate your method  \n  \nOther comments:  \n- [x] The conclusion is for me the strongest part of the article. Therefore, I would love to see more of the conclusion in the main text, and in the introduction (in which you can announce the main argument of the paper more explicitly)  \n- [x] The author introduces new results and figures in the conclusion. I suggest to move these results and figures to the results' section.  \n- [x] Minor detail: In the conclusion, it becomes clear that the author did not finish the research on/for the method yet ('preliminary'). This frames the paper a bit as a work-in-progress paper which is not finished yet. Re-phrase it e.g. by stating that the first part of the (evaluation of the) method is finished and presented in this article, and that there will be follow-up research in another paper. Also remove the 'preliminary' from the conclusion's title.  \n  \n  \nReviewer: 2  \n  \nComments to the Author  \nI do think the author has sufficiently improved the article and I recommend publishing it. I only have minor remarks to the response and discussion of cultural analytics in the article (also in light of the other reviewer's comments), notably that the author considers their work only \"tangentially related\" to cultural analytics. I am not entirely convinced by this (also because the discussion of cultural analytics remains cursory in the article and indeed does leave out Arnold and Tilton's work which certainly does reflect a more complex approach/definition of Cultural Analytics than the one offered in the article - yet again, I do think the article can be published as is in spite of this).\n\n\n\n~~More testing and development are needed to fully gauge the limits and possibilities of this way of producing computational supercuts.~~\n\n~~I expect there will be more supercuts available in the same online repository at the time of publication~~\n\n~~The supercut is a palimpsest in structure ‚Äïan artefact of artefacts‚Äï and, as Tohline argues, a material instantiation of algorithmic thinking. In its computational form, I submit it can also be thought of as an audiovisual programming interface: a way of performing data-science through images, in which the techniques of film editing are used to interrogate machine learning operations such as data sampling, feature extraction, and error optimisation. If through the mass datafication of culture the film archive became a database, through machine learning this database is now a computational model. The question before us now is not only how to analyse moving images using computational methods, but more broadly, how to interrogate, call to account, and intervene, the computational modelling of visual culture at large. In other words, how to live with and among models that simultaneously reduce and expand what can be seen and known at specific historical junctures.~~\n\n\nFrom ChatGpt:\n\nexplain cultural analytics, quantitative film analysis, and computational humanities, including distant viewing, for a literature review in a research paper\n\n\u003eCultural Analytics, Quantitative Film Analysis, and Computational Humanities are interdisciplinary fields that use computational tools and methods to analyze and study cultural expressions and artifacts such as literature, film, and visual arts. They aim to understand the patterns, trends, and relationships among different cultural expressions and to provide insights into the cultural and historical context in which they were produced.\n\n\n~~\u003eQuantitative Film Analysis is a subfield of Cultural Analytics that focuses on the use of computational methods to analyze film data. This includes the analysis of visual and audio features of films, such as shot length, color palette, and sound design, as well as the analysis of textual data, such as scripts and reviews. Quantitative Film Analysis is used to study film style, genre, and representation, and to provide insights into the historical and cultural context in which films were produced.~~\n\n\u003eComputational Humanities is an interdisciplinary field that uses computational tools and methods to study the humanities. This includes the use of computational techniques to analyze literary texts, visual arts, music, and film. Computational Humanities seeks to provide new ways of understanding cultural expressions and artifacts and to shed light on the cultural and historical context in which they were produced.\n\n\u003eDistant viewing is a method in computational humanities that uses computational techniques to analyze large collections of cultural data without close examination of individual items. The goal of distant viewing is to identify patterns, trends, and relationships at a large scale and to provide a quantitative understanding of cultural expressions. This method allows for the analysis of vast amounts of cultural data in a systematic and objective manner, providing new insights into the cultural and historical context of cultural expressions.\n\n\u003eIn conclusion, Cultural Analytics, Quantitative Film Analysis, and Computational Humanities are interdisciplinary fields that use computational tools and methods to study cultural expressions and artifacts. They provide new ways of understanding cultural phenomena and provide insights into the cultural and historical context in which they were produced. The use of distant viewing in computational humanities is a valuable method for analyzing large collections of cultural data and provides a quantitative understanding of cultural expressions at scale.\n\n~~Videographic criticism and the video essay are emerging scholarly practices in media studies that use the medium of video to analyze, interpret, and critique media artifacts, such as films, television shows, and video games. These practices represent a new form of media analysis that combines traditional critical methods, such as close reading and textual analysis, with audiovisual elements, such as editing, sound design, and animation.\n\n~~Catherine Grant, Christian Keathley, and Jason Mittell are leading scholars in the field of videographic criticism and the video essay. Catherine Grant is known for her work on the theory and practice of videographic film and moving image studies. In her work, she explores the potential of videographic criticism to enrich our understanding of film and moving images through the use of audiovisual elements.\n\n~~Christian Keathley is a film scholar who has made significant contributions to the development of videographic criticism as a scholarly practice. He is particularly interested in the relationship between the moving image and the viewer, and how videographic criticism can provide new ways of understanding the experience of watching films.\n\n~~Jason Mittell is a media scholar who has made important contributions to the field of videographic criticism and the video essay. In his work, he explores the use of video as a tool for critical analysis and the ways in which video can be used to tell stories and make arguments about media artifacts. Mittell is also interested in the potential of the video essay to foster a new form of public scholarship that combines the academic rigor of traditional scholarship with the accessibility and engagement of digital media.\n\n~~In conclusion, videographic criticism and the video essay are emerging scholarly practices in media studies that use the medium of video to analyze and critique media artifacts. These practices represent a new form of media analysis that combines traditional critical methods with audiovisual elements. Scholars such as Catherine Grant, Christian Keathley, and Jason Mittell have made significant contributions to the development of these practices and have explored their potential to enrich our understanding of media artifacts and foster a new form of public scholarship.~~\n\n~~But with the system in place, I expect to render more supercuts and make them available in the same online repository by the time of publication. I also plan two follow-up pieces of research to further explore the epistemic and aesthetic potential of the computational video essay.~~\n\nResponse:\n\nTo both reviewers:\nThank you for your comments and valuable feedback.\nI revised the text accordingly, and found especially useful your comments about clarifying the contribution in relation to cultural analytics. This prompted a more thorough revision than I had initially anticipated, made me go back to recover additional sources I had left in out in the initial draft, and helped me adjust the title of the piece to better reflect my own position.\nI also included two new figures to illustrate the main arguments in the text, and a filmography in the references section.\nI believe the balance between theoretical and processual writing is better and the piece clearer as a result. Please find my response on the specific points you raised below, and a revised version of the article attached.\n\nIn response to reviewer 1\n\n- I have elaborated on the theoretical framework, expanding on the ideas of the main authors I engage with, and re-framing the review of literature to better clarify my own contribution in relation to cultural analytics, which I also hope is now more evident with the adjusted title of the piece.\n- To address the structural points raised about the last part of the text, I reorganised the \"preliminary results\" and \"conclusions\" sections into  \"findings and evaluation\" and a \"future research\". This clarifies for the reader what research was already carried out, the findings of it, and future work in the field, I think.\n- I moved several points from the conclusions to the introduction, as suggested, to better signpost the overall contribution of the piece.\n\nIn response to reviewer 2\n\n- I engage more with cultural analytics, I also make a more explicit reference to Arnold and Tilton's \"distant viewing\" work, in both the literature review and the findings and evaluations section.\n- I also made explicit the relation my approach has with existing  cultural analytics methods and practices, including a clearer distinction of epistemic modalities and how these are linked through inductive computing.\n","lastmodified":"2023-07-02T17:28:23.549113959Z","tags":null},"/public_notes/Creanalytics":{"title":"Creanalytics","content":"---\n\n_Creanalytics_: a portmanteau of creative and analytic.\n\nThis idea came up when writing about the[[private/To do/Towards a Computational Video Essay | computational video essay]], largely in response to creative and analytic research methods coexisting under a unified computational framework.   In the case of deep learning, this could be seen as _generation_ through _prediction_, similar to the [analytic-synthetic distinction](https://www.williamcotton.com/articles/chatgpt-and-the-analytic-synthetic-distinction) which I found in this piece about [analytic augmentation in prompt engineering](https://github.com/williamcotton/empirical-philosophy/blob/main/articles/from-prompt-alchemy-to-prompt-engineering-an-introduction-to-analytic-agumentation.md).\n\nUpdate: this is now an article! Forthcoming in a special Issue on critical technical practice in the journal [Convergence: The International Journal of Research into New Media Technologies](https://journals.sagepub.com/home/con) \n\n![[assets/images/Screenshot from 2023-07-02 18-08-10.png]]\n\nAlso related to a project in collaboration with my wonderful colleagues [Jonathan Gray](https://jonathangray.org/) and [Mercedes Bunz](https://www.kcl.ac.uk/people/mercedes-bunz) on using video editing techniques as data analysis. We commissioned [Sam Lavigne](https://lav.io/) to work on  his [[public_notes/Tools/Videogrep|Videogrep]] tool! More on this soon!","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Creative-AI-Theory-and-Practice":{"title":"Creative AI Theory and Practice","content":"---\n\nThe recording of our [symposium on Creative AI](https://www.kcl.ac.uk/events/creative-ai-theory-and-practice) is now available. Presentations are indexed at different timestamps.\n\n\u003eOn Friday 27 January 2023, Creative AI Lab at King‚Äôs College London/Serpentine (Professor Mercedes Bunz and curator Eva J√§ger as Lab‚Äôs co-founders, Dr Daniel Ch√°vez Heras, PhD student Alasdair Milne, Professor Joanna Zylinska) hosted a one-day symposium supported by the King‚Äôs Institute for Artificial Intelligence.\n\n\n\u003ciframe title=\"Creative AI: Theory and Practice\" src=\"https://www.youtube.com/embed/xuZsf3ZX7k8?feature=oembed\" height=\"113\" width=\"200\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n\n\n\n","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Cultural-Metabolism-and-LLMs-on-wheels":{"title":"NYU - Prague","content":"---\n\nI was invited by the great people of the [Digital Theory Lab at NYU](https://digitalhumanities.nyu.edu/projects/digital-theory-lab/) to present this in Prague this summer. Very much looking forward to it!\n\n# Cultural Metabolism and LLMs on wheels\n## Abstract\nLarge Language Models ( #LLMs ) trained on vast amounts of internet text are able to simulate natural language structures to a degree that enables novel interfaces for human-computer interaction. One key function in these systems can be observed at the micro level, when the next token in an unfolding series is predicted and concatenated recursively, in other words, at the logical step when calculation becomes synthesis.\nIn this presentation I focus on this very localised but crucial moment of exchange, drawing a parallel with metabolic processes in cellular biology, to explore the notion of self-contained and embedded autonomous systems: LLMs+ or LLMs on wheels.\n\nThis is related to my interest in agent-based modelling and complex systems in cultural domains. Inspired among other influences by [Thilo Gros](https://reallygross.de/me) and his work at [Biond Lab](https://biond.org/)\nI love his [complexity papers videos](https://www.youtube.com/@complexitypapers)!\n\n\n\u003ciframe title=\"Estimation of functional diversity and species traits from ecological monitoring data\" src=\"https://www.youtube.com/embed/BsNkfj0HZBs?feature=oembed\" height=\"113\" width=\"200\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Human-centered-machine-vision":{"title":"Human-centered machine vision?","content":"---\n\nFrom the _Click and Collect: Show Me Your Dataset_ event at Somerset House. I was invited as part of a panel with [Charlotte Webb](https://www.feministinternet.com/) and [Kristina Pulejkova](https://kristinapulejkova.com/) to discuss human-centred design in relation to AI. \n\nI presented ongoing research on [[public_notes/Creanalytics|Creanalytics]] forthcoming as an article in a special issue on _Critical Technical Practice_ in the journal [Convergence](https://journals.sagepub.com/home/con):\n\n\u003ciframe title=\"Daniel Ch\u0026amp;aacute;vez Heras: Artist Presentation\" height=\"240\" width=\"426\" src=\"https://player.vimeo.com/video/806299101?h=31224a9af9\u0026amp;app_id=122963\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.775 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n---\n\nThe recording of the panel is also now available here starting at about 6:14\n\n\u003ciframe title=\"vimeo-player\" src=\"https://player.vimeo.com/video/806397141?h=0470a6a1ef\" #t=22s?autoplay=0 width=\"640\" height=\"360\" frameborder=\"0\"    allowfullscreen\u003e\u003c/iframe\u003e\n\n--- \n\nSee the  [twitter thread](https://twitter.com/sh_studios_/status/1626900453828005889?s=20) for more on the larger event.\n\n![[assets/FpPpVlVWIAE1uFq.jpeg]]","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/The-Digital-Pastoral":{"title":"The Digital Pastoral","content":"---\n\nI created the image below using #stablediffusion (before it was cool and every media scholar started doing it) and used it for my very short presentation at [[Transmediale 2023 | Transmediale 2023]].\n\n\n![[assets/minor_tech_DCH.jpg]]\n\nI can't recall the exact prompt, but I do remember I referenced the illustrations on [Jakub R√≥≈ºalski](https://jrozalski.com/) whom I discovered through [Scythe](\u003chttps://en.wikipedia.org/wiki/Scythe_(board_game)\u003e).\n\nAnd here's [a link to the newspaper](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf)!","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Tools/Mermaid-diagrams":{"title":"Mermaid diagrams","content":"\n\u003eJavaScript based diagramming and charting tool that renders Markdown-inspired text definitions to create and modify diagrams dynamically.\n\nIt has a [live editor](https://mermaid.live/) for non-programmers, and has [some tutorials](https://mermaid.js.org/config/Tutorials.html). Maybe relevant for students. There's a Jupyter integration too. Interesting for #criticaltechnicalpractice \nMaybe I can use this to render an maintain a web version of the [[private/Projects/Large/VANGE]] diagram in my office.\n\n![Mermaid](https://mermaid.js.org/header.png)\n\n","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Tools/Motion-Canvas":{"title":"Motion Canvas","content":"---\n\n\u003eVisualize Complex Ideas Programmatically.\n\u003eIt's a specialized tool designed to create informative vector animations and synchronize them with voice-overs. It's not meant to be a replacement for traditional video editing software.\n\nProcedural animation tool, that can be integrated with a web-editor to sync sound and produce animated videos. Great for education, documentation, and reflection in #criticaltechnicalpractice Reminds me a bit of Flash action script, but #FOSS and nicer for rapid development and to comunicate abstract ideas precisely. Think of [Freya Holm√©r](https://www.youtube.com/@Acegikmo) videos or [Ms Coffee Beans](https://www.youtube.com/@AICoffeeBreak).\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/H5GETOP7ivs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Tools/Streaming-Video-Model":{"title":"Streaming Video Model","content":"---\n\n[Interesting video model](https://arxiv.org/abs/2303.17228#) that accounts for frames and sequences in a unified model. \nAbstract:\n[](https://arxiv.org/search/cs?searchtype=author\u0026query=Zha%2C+Z)\n\n\u003e Video understanding tasks have traditionally been modeled by two separate architectures, specially tailored for two distinct tasks. Sequence-based video tasks, such as action recognition, use a video backbone to directly extract spatiotemporal features, while frame-based video tasks, such as multiple object tracking (MOT), rely on single fixed-image backbone to extract spatial features. In contrast, we propose to unify video understanding tasks into one novel streaming video architecture, referred to as Streaming Vision Transformer (S-ViT). S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks. Then the frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks. The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. We believe that the concept of streaming video model and the implementation of S-ViT are solid steps towards a unified deep learning architecture for video understanding. Code will be available at [this https URL](https://github.com/yuzhms/Streaming-Video-Model).\n\n","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Tools/Videogrep":{"title":"Videogrep","content":"---\n\n\u003eVideogrep is a command line tool that searches through dialog in video files and makes [supercuts](https://vimeo.com/440746435) based on what it finds. It will recognize .srt or .vtt subtitle tracks, or transcriptions that can be generated with vosk, pocketsphinx, and other tools.\n\nSam posted an implementation of Videogrep that runs on Colab, which made me think of running this on a Jupyter Lab instance from our College's computer cluster, and then possibly using it for teaching. The college does not host its own notebook instance, as far as I know, but the cluster can serve a headless Jupyter that can be \"tunnelled in\" through ssh.\n\nHere's a supercut of [recent lecture](https://www.youtube.com/watch?v=b6ogLgWnpes) by  [Shannon Mattern](https://cinemastudies.sas.upenn.edu/people/shannon-mattern) at KCL called \"Modeling Doubt, Coding Humility: A Speculative Syllabus\"\n\n\u003ciframe title=\"Doubt ‚Äï with Shannon Mattern\" height=\"240\" width=\"426\" src=\"https://player.vimeo.com/video/830237949?h=5fb9301c4d\u0026amp;app_id=122963\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.775 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n\nInterestingly, not one single mention of \"code\" or \"coding\", nor of \"humility\". I recommend watching the whole talk, though.\n\nHere's the [repository](https://github.com/antiboredom/videogrep)for Videogrep. Here's the [Colab minimal example](https://t.co/QGKTLxOZ52).\nAnd here are other examples from Sam's repo:\n\n-   [silence extraction](https://github.com/antiboredom/videogrep/blob/master/examples/only_silence.py)\n-   [automatically creating supercuts](https://github.com/antiboredom/videogrep/blob/master/examples/auto_supercut.py)\n-   [creating supercuts based on youtube searches](https://github.com/antiboredom/videogrep/blob/master/examples/auto_youtube.py)\n-   [creating supercuts from specific parts of speech](https://github.com/antiboredom/videogrep/blob/master/examples/parts_of_speech.py)\n-   [creating supercuts from spacy pattern matching](https://github.com/antiboredom/videogrep/blob/master/examples/pattern_matcher.py)","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Tools/Zeeschuimer":{"title":"Zeeschuimer","content":"---\n\n\u003e Zeeschuimer is a browser extension that monitors internet traffic while you are browsing a social media site, and collects data about the items you see in a platform's web interface for later systematic analysis. Its target audience is researchers who wish to systematically study content on social media platforms that resist conventional scraping or API-based data collection.\n\n\n![Zeeschuimer](https://github.com/digitalmethodsinitiative/zeeschuimer/blob/master/images/example_screenshot.png?raw=true)\n\nThis would definitely be of interest to some of my UX students as a way to integrate in their usability tests and cultural probes when analysing social media usage.\nRepository [here](https://github.com/digitalmethodsinitiative/zeeschuimer).\n\n\n\n\n\n\n","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/Transmediale-2023":{"title":"Transmediale 2023","content":"---\n\nI attended [Transmediale](https://2023.transmediale.de/en) and gave a very small presentation as part of the panel *Toward Minor Tech*.\n\n[![Towards a Minor Tech Newspaper](assets/images/minor_tech_news.jpeg)](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf)\n\nThis is the result of a workshop organised by by Geoff Cox and Christian Ulrik Andersen every year as part of [SHAPE Digital Citizenship](https://shape.au.dk/en/) \u0026 [Digital Aesthetics Research Center, Aarhus University](https://darc.au.dk/) and [Centre for the Study of the Networked Image, London South Bank University](https://www.centreforthestudyof.net/), this year in collaboration with [King‚Äôs College London](https://www.kcl.ac.uk/). \n\nThe workshop took place over several days in London and inlcuded public talks from Marloes de Valk on the [damaged earth catalog](https://damaged.bleu255.com/) and from Tung-Hui Hu on [digital lethargy](https://thephotographersgallery.org.uk/whats-on/talk-digital-lethargy).\nMy contribution to the publication was titled: *A minor critique of minor tech* and can be found as a PDF, along with all other contirbutions, by clicking [here](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf).\n\n![minor tech panel](assets/images/minor_tech_panel.jpeg)\n","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null},"/public_notes/small-large-LLMs":{"title":"small large LLMs","content":"---\nMy colleague [Mercedes Bunz](https://mercedesbunz.net/) made me aware of this allegedly [leaked document](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) from a Google engineer, in which they make the case for open-sourcing their models. TLDR; The argument is that owning and cultivating the ecosystem for innovation is more valuable than keeping the models fenced off.\n\nCaveats notwithstanding, say for instance that the diminishing value of training does not account for people's salaries in publicly-funded institutions, it is still an interesting read, especially the timeline narrating all the developments. Good for teaching, but also to make sense of the various recent moves in the field.\n\nSome of the models and mentioned:\n[LLaMA](https://arxiv.org/pdf/2303.16199.pdf) ‚Äï Meta\n[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)‚Äï Stanford\n[Alpaca LoRA](https://github.com/tloen/alpaca-lora)‚Äï Stanford + Eric Wang. See [paper here](https://arxiv.org/pdf/2106.09685.pdf).\n[A Chatbot interface for Alpaca](https://github.com/deep-diver/LLM-As-Chatbot)\n[Dolly 15k instructions dataset](https://huggingface.co/datasets/c-s-ale/dolly-15k-instruction-alpaca-format)\n\u003e `databricks-dolly-15k` is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language models to exhibit the magical interactivity of ChatGPT.\n\n[GPT4all](https://github.com/nomic-ai/gpt4all)‚Äï open stack pipeline based on JPT-J and LLaMA\n[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)\n\u003ean open-source Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT\n\nThis las one was developed by a student-led university consortium called [LMSYS Org](https://lmsys.org/about/)!\n\n","lastmodified":"2023-07-02T17:28:23.553113955Z","tags":null}}