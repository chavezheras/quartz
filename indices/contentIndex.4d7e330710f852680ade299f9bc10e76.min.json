{"/":{"title":"üìù Daniel's Notes.","content":"\nWelcome to my public notes. Not quite a blog, but not a timeline either. This kind of personal wiki of linked notes is my attempt to gradually move away from social media platforms but still keep some kind of connection with the public internet.\n\nThis \"net of notes\" is very close to the idea of a digital garden, in that it takes inspiration from the early web; from [hypertext](http://www.eastgate.com/garden/Enter.html) and the [Memex](https://en.wikipedia.org/wiki/Memex), and from keeping some kind of ownership of the files one shares. In keeping with these ideas, the notes in this garden are often drafts that are meant to grow (or wither) organically. Some notes might make it to my main site as they grow into projects or publications, many will remain here while they grow.\n\nYou can read [A Brief History \u0026 Ethos of the Digital Garden](https://maggieappleton.com/garden-history), and this piece on [The Garden and the Stream](https://hapgood.us/2015/10/17/the-garden-and-the-stream-a-technopastoral/) to find out more about digital gardens.\n\n---\n\n","lastmodified":"2023-08-10T10:28:46.160410948Z","tags":null},"/.ipynb_checkpoints/_index-checkpoint":{"title":"üìù Daniel's Notes.","content":"\nWelcome to my public notes. Not quite a blog, but not a timeline either, this kind of personal wiki of linked notes is an attempt to gradually move away from social media platforms but still keep some kind of connection with the public internet. It is very close to the idea of a digital garden, in that it takes inspiration from the early web; from hypertext and the memex, and and from owning the files one shares. In keeping with these ideas, the notes in this garden are never fully finished and are meant to grow (or wither) organically. Read A Brief History \u0026 Ethos of the Digital Garden, and The Garden and the Stream to know more about digital gardens.\n","lastmodified":"2023-08-10T10:28:46.160410948Z","tags":null},"/public_notes/5-principles-of-life":{"title":"5 principles of life","content":"---\n\nSome very high-level background about cell-biology for my talk on  [[public_notes/Cultural Metabolism and LLMs on wheels|Cultural Metabolism and LLMs on wheels]] at Prague earlier this year.\n\nAccording to Paul Nurse:\n\n1. Living things are bounded physical entities\n2. The bounded entity is the chemical and informational machine\n3. This chemical and informational machine has a hereditary system\n\t1. The system prefigures how the entity works\n\t2. But has in-built variability to respond to its environment ([epigenetics](https://en.wikipedia.org/wiki/Epigenetics))\n4. The entity can therefore evolve by natural selection\n5. The living thing can have purpose of being better adapted to the life state it finds itself\n\nFrom his book [What is Life?](https://www.waterstones.com/book/what-is-life/paul-nurse/9781788451406)\n\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Creanalytics":{"title":"Creanalytics","content":"---\n\n_Creanalytics_: a portmanteau of creative and analytic.\n\nThis idea came up when writing about the computational video essay, largely in response to creative and analytic research methods coexisting under a unified computational framework in the age of generative AI. This could be understood as _generation_ through _prediction_, similar to the [analytic-synthetic distinction](https://www.williamcotton.com/articles/chatgpt-and-the-analytic-synthetic-distinction) which I found in this piece about [analytic augmentation in prompt engineering](https://github.com/williamcotton/empirical-philosophy/blob/main/articles/from-prompt-alchemy-to-prompt-engineering-an-introduction-to-analytic-agumentation.md).\n\nUpdate: this is now an article! Forthcoming in a special Issue on critical technical practice in the journal [Convergence: The International Journal of Research into New Media Technologies](https://journals.sagepub.com/home/con) \n\n![[assets/images/Screenshot from 2023-07-02 18-08-10.png]]\n\nAlso related to a project in collaboration with my wonderful colleagues [Jonathan Gray](https://jonathangray.org/) and [Mercedes Bunz](https://www.kcl.ac.uk/people/mercedes-bunz) on using video editing techniques as data analysis. We commissioned [Sam Lavigne](https://lav.io/) to work on  his [[public_notes/Tools/Videogrep|Videogrep]] tool! More on this soon!","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Creative-AI-Theory-and-Practice":{"title":"Creative AI Theory and Practice","content":"---\n\nThe recording of our [symposium on Creative AI](https://www.kcl.ac.uk/events/creative-ai-theory-and-practice) is now available. Presentations are indexed at different timestamps.\n\n\u003eOn Friday 27 January 2023, Creative AI Lab at King‚Äôs College London/Serpentine (Professor Mercedes Bunz and curator Eva J√§ger as Lab‚Äôs co-founders, Dr Daniel Ch√°vez Heras, PhD student Alasdair Milne, Professor Joanna Zylinska) hosted a one-day symposium supported by the King‚Äôs Institute for Artificial Intelligence.\n\n\n\u003ciframe title=\"Creative AI: Theory and Practice\" src=\"https://www.youtube.com/embed/xuZsf3ZX7k8?feature=oembed\" height=\"113\" width=\"200\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n\n\n\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Cultural-Metabolism-and-LLMs-on-wheels":{"title":"NYU - Prague","content":"---\n\nI was invited by the great people of the [Digital Theory Lab at NYU](https://digitalhumanities.nyu.edu/projects/digital-theory-lab/) to present this in Prague this summer. Very much looking forward to it!\n\n# Cultural Metabolism and LLMs on wheels\n## Abstract\nLarge Language Models ( #LLMs ) trained on vast amounts of internet text are able to simulate natural language structures to a degree that enables novel interfaces for human-computer interaction. One key function in these systems can be observed at the micro level, when the next token in an unfolding series is predicted and concatenated recursively, in other words, at the logical step when calculation becomes synthesis.\nIn this presentation I focus on this very localised but crucial moment of exchange, drawing a parallel with metabolic processes in cellular biology, to explore the notion of self-contained and embedded autonomous systems: LLMs+ or LLMs on wheels.\n\nThis is related to my interest in agent-based modelling and complex systems in cultural domains. Inspired among other influences by [Thilo Gros](https://reallygross.de/me) and his work at [Biond Lab](https://biond.org/)\nI love his [complexity papers videos](https://www.youtube.com/@complexitypapers)!\n\n\n\u003ciframe title=\"Estimation of functional diversity and species traits from ecological monitoring data\" src=\"https://www.youtube.com/embed/BsNkfj0HZBs?feature=oembed\" height=\"113\" width=\"200\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Human-centered-machine-vision":{"title":"Human-centered machine vision?","content":"---\n\nFrom the _Click and Collect: Show Me Your Dataset_ event at Somerset House. I was invited as part of a panel with [Charlotte Webb](https://www.feministinternet.com/) and [Kristina Pulejkova](https://kristinapulejkova.com/) to discuss human-centred design in relation to AI. \n\nI presented ongoing research on [[public_notes/Creanalytics|Creanalytics]] forthcoming as an article in a special issue on _Critical Technical Practice_ in the journal [Convergence](https://journals.sagepub.com/home/con):\n\n\u003ciframe title=\"Daniel Ch\u0026amp;aacute;vez Heras: Artist Presentation\" height=\"240\" width=\"426\" src=\"https://player.vimeo.com/video/806299101?h=31224a9af9\u0026amp;app_id=122963\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.775 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n---\n\nThe recording of the panel is also now available here starting at about 6:14\n\n\u003ciframe title=\"vimeo-player\" src=\"https://player.vimeo.com/video/806397141?h=0470a6a1ef\" #t=22s?autoplay=0 width=\"640\" height=\"360\" frameborder=\"0\"    allowfullscreen\u003e\u003c/iframe\u003e\n\n--- \n\nSee the  [twitter thread](https://twitter.com/sh_studios_/status/1626900453828005889?s=20) for more on the larger event.\n\n![[assets/FpPpVlVWIAE1uFq.jpeg]]","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Andrea-Farina":{"title":"Andrea Farina","content":"\n\n## About\nAndrea Farina is a PhD Researcher in Digital Humanities. So far, he has been involved in the syntactic or semantic annotation for the following on-going projects: [the Greek WordNet](https://greekwordnet.chs.harvard.edu), [the Latin WordNet](https://latinwordnet.exeter.ac.uk), [the Sanskrit WordNet](https://sanskritwordnet.unipv.it), and [the Digital Corpus of Sanskrit](http://www.sanskrit-linguistics.org/dcs/index.php). He currently holds a UKRI scholarship from the London Arts \u0026 Humanities Partnership (LAHP).\n\n### Background\nLinguistics, Classics.\n\n### Field\n#ComputationalLinguistics\n\n### Interests/projects/skills\n- Computational tools and methods for Historical Linguistics.\n- Syntactic and semantic annotation and quantitative analysis in ancient languages (Latin, Ancient Greek).\n- Analysis of the semantics of pre-verbs via computational methods in ancient languages (Latin, Ancient Greek).\n- Analysis of literal and metaphorical motion events.\n\n## Links\nhttps://www.kcl.ac.uk/people/andrea-farina\nhttps://twitter.com/ImAndreaFarina","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Annotation-guidelines":{"title":"Annotation guidelines","content":"---\n\nDuring day two, [[public_notes/Sculpting Time with Computers/Mila Oiva|Mila]] and [[public_notes/Sculpting Time with Computers/Andrea Farina|Andrea]] worked on a series of guidelines for human annotation of the the soviet news reels, focusing on scenes or \"blocs\", rather than stories.\nTags include:\n\n- Time stamps of the blocs\n- Topics of the blocs\n- Settings or locations\n- Sound change over blocs\n\nThese were thought as a possible new sources of data from human annotators, but also as sampling guidelines to inform the list of [[public_notes/Sculpting Time with Computers/day two|CLIP queries]].\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Bel%C3%A9n-Vidal":{"title":"Bel√©n Vidal","content":"\n## About\nBel√©n Vidal is Reader in Film Studies. She is the author of [Figuring the Past: Period Film and the Mannerist Aesthetic](http://www.oapen.org/search?identifier=426536) (Amsterdam University Press, 2012) and [Heritage Film. Nation, Genre and Representation](https://cup.columbia.edu/book/heritage-film/9780231162036) (Columbia University Press/Wallflower Press, 2012). She is the co-editor of [The Biopic in Contemporary Film Culture](https://www.routledge.com/The-Biopic-in-Contemporary-Film-Culture/Brown-Vidal/p/book/9780415899413) (AFI Film Readers, Routledge 2014) and of _Cinema at the Periphery_ (Wayne State University Press, 2010).\n\n### Background\nShe holds a PhD in Film Studies from Glasgow University, an MA from Georgetown University and a BA (Honors) from the University of Valencia. Before joining King‚Äôs in 2008, she was Lecturer in Film Studies at the University of St Andrews.\n\n### Field of study\n#FilmStudies\n\n### Interests/projects/skills\n\nBel√©n co-leads the international project [_AGE-C Ageing and Gender in European cinema_](https://wordpress.er.kcl.ac.uk/film-studies-research/2022/11/age-c-ageing-and-gender-in-european-cinema/)\n\n\u003eAGE-C aims to establish cultural gerontology as a key approach in film studies by training a cohort of postdocs to study how cinematic representations of gender shape notions of old age and well-being across Europe.\n\n## Links\nhttps://www.kcl.ac.uk/people/dr-belen-vidal\n\n\n#### Research Interests¬†\n\nFilm analysis and film theory, focusing on:\n- History and memory in contemporary cinemas\n- The historical film genres, especially the biopic and the heritage film\n- Spanish film history, and contemporary cinema \u0026 media in Spain\n- Cinephilia and film cultures","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Carlo-Bretti":{"title":"Carlo Bretti","content":"\n\n## About\nI‚Äôm a first-year PhD candidate at the [Multimedia Analytics Lab](https://multix.io) of the University of Amsterdam under the supervision of [Nanne van Noord](https://nanne.github.io) and [Pascal Mettes](https://staff.fnwi.uva.nl/p.s.m.mettes/).\n\nI‚Äôm currently conducting research on computer vision and deep learning for film production and analysis. Before starting at MultiX, I worked on automatic trailer generation at the [Video \u0026 Image Sense Lab](https://ivi.fnwi.uva.nl/vislab/). Prior to that, I wrote my thesis on zero-shot action recognition in videos using diverse sets of object-scene compositions, advised by Pascal Mettes.\n\n### Background\n- MSc Data Science, 2021\n    University of Amsterdam\n\n- BSc Communication Science, 2020\n    University of Amsterdam\n\n### Field\n#ComputerVision \n\n### Interests/projects/skills\n- Computer Vision\n- Creative AI\n- Video Understanding\n\n## Links\nhttps://carlobretti.github.io/","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/High-dimensional-cinema":{"title":"panel on high-dimensional cinema","content":"---\n\nAs a public-facing part of the [[public_notes/Sculpting Time with Computers/Sculpting Time With Computers|Sculpting Time With Computers workshop]], in this ‚Äúmeeting of the labs‚Äù event, [[public_notes/Sculpting Time with Computers/Nanne van Noord|Nanne van Noord]], [[public_notes/Sculpting Time with Computers/Mila Oiva|Mila Oiva]] and myself got together to to present recent research and engage in conversation about recent advances at the intersection between cultural analytics, computational aesthetics, and machine learning.\n\nThe panel was chaired by [[public_notes/Sculpting Time with Computers/Bel√©n Vidal|Bel√©n Vidal]] and it was recorded. I'll post the video here when it is available.\n\n![[assets/images/high-dimensional_cinema.jpeg]]","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Isadora-Campregher":{"title":"Isadora Campregher","content":"\n## About\nResearch Associate and Lecturer at Goethe University. Research associate [@DiciHub](https://twitter.com/DiciHub). Producer [@Authoredby_AI](https://twitter.com/Authoredby_AI). Amateur data scientist.\n\n### Background\nInternational Relations and MA in Sociology with a focus on Gender Studies from Universidade Federal do Rio Grande do Sul (UFRGS) in Porto Alegre, Brazil.  Master in Audiovisual and Cinema Studies (IMACS) based at the Goethe University.\n\n### Field\n#FilmStudies #FilmIndustries\n\n### Interests/projects/skills\nCinema industries, gender, and data science.\n\n## Links\nhttps://de.linkedin.com/in/isadora-campregher-paiva-b59072124\nhttps://twitter.com/Isa_CamPaiva\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Jake-Berger":{"title":"Jake Berger","content":"\n\n## About\nJake is Executive Product Manager of BBC Archive.\n\n### Background\nI'm not sure!\nJake, if you are reading this and want to add something here, give me a shout.\n\n### Field\n#Archives #DigitalPreservation \n\n### Interests/projects/skills\n[BBC Rewind](https://bbcrewind.co.uk/), [Genome](https://genome.ch.bbc.co.uk/), [RemArc](https://remarc.bbcrewind.co.uk/), [BBC SFX](https://sound-effects.bbcrewind.co.uk/)\n\n## Links\nhttps://www.bbc.co.uk/archive/stuff-we-do","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Joel-McKim":{"title":"Joel McKim","content":"\n\n## About\nJoel's work focuses on the study of digital images and the impact of digital technologies on architecture, art and design. He is the Director of the [Vasari Research Centre for Art and Technology](http://www.bbk.ac.uk/vasari/) and I'm currently a Visiting Research Fellow at the [V\u0026A Research Institute](https://www.vam.ac.uk/info/the-va-research-institute-vari)where he is working on a project entitled \"A Prehistory of Machine Vision: Exploring the V\u0026A Computer Art Collection\".\n\n### Background\nJoel is Senior Lecturer, Department of Film, Media \u0026 Cultural Studies at Birkbeck.  Before that he was a Kenneth P. Dietrich Post-Doctoral Fellow at the University of Pittsburgh in the Department of the History of Art and Architecture and an FQRSC Post-Doctoral Fellow at McGill University in the Department of Art History and Communication Studies, where he participated in the Media and Urban Life research group. He was also a full-time Lecturer in the Department of Communication Studies at Concordia University in Montreal.\n\n### Field\n#DigitalAnimation , #Architecture and #Design \n\n### Interests/projects/skills\nJoel's previous professional experience includes web design and internet analytics.\nHe is currently the co-director of the [MA Digital Media](http://www.bbk.ac.uk/study/2017/postgraduate/programmes/TMADIGMC_C/).\n\n## Links\nJoels' first book¬†_Architecture, Media and Memory: Confronting Complexity in Post-9/11 New York_¬†was published last year by Bloomsbury. \nhe is working on a second book entitled¬†_Rendered: Digital Animation in Art, Architecture and Design_, which draws from an ongoing investigation of the rapidly expanding field of digital animation, undertaken in collaboration with Esther Leslie. This animation project has thus far included the BIH sponsored [Life Remade: Politics of Animation Symposium](https://birkbeckliferemade.wordpress.com/), an ongoing BIMI/Vasari sponsored digital animation screening series, and a 2017 special issue of the journal¬†animation¬†entitled \"[Life Remade: Critical Animation in the Digital Age](http://journals.sagepub.com/toc/anma/12/3).\"\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/List-of-all-notes":{"title":"a list of all notes here","content":"---\nHere is a list of all notes (excluding [[public_notes/Sculpting Time with Computers/Participants|participants]]) that together give an account of our activities at the [[public_notes/Sculpting Time with Computers/Sculpting Time With Computers|Sculpting Time With Computers]] workshop.\n\n- [[public_notes/Sculpting Time with Computers/Annotation guidelines|Annotation guidelines]]\n- [[public_notes/Sculpting Time with Computers/High-dimensional cinema|High-dimensional cinema]]\n- [[public_notes/Sculpting Time with Computers/List of all notes|List of all notes]]\n- [[public_notes/Sculpting Time with Computers/Participants|Participants]]\n- [[public_notes/Sculpting Time with Computers/Reverse compression as motion estimation|Reverse compression as motion estimation]]\n- [[public_notes/Sculpting Time with Computers/cinematic time|cinematic time]]\n- [[public_notes/Sculpting Time with Computers/collections|collections]]\n- [[public_notes/Sculpting Time with Computers/computational|computational]]\n- [[public_notes/Sculpting Time with Computers/day one|day one]]\n- [[public_notes/Sculpting Time with Computers/day two|day two]]\n- [[public_notes/Sculpting Time with Computers/design|design]]\n- [[public_notes/Sculpting Time with Computers/film|film]]\n- [[public_notes/Sculpting Time with Computers/high-level reasoning about time|high-level reasoning about time]]\n- [[public_notes/Sculpting Time with Computers/historical time|historical time]]\n- [[public_notes/Sculpting Time with Computers/ideas and next steps|ideas and next steps]]\n- [[public_notes/Sculpting Time with Computers/long list of ideas|long list of ideas]]\n- [[public_notes/Sculpting Time with Computers/subjective time|subjective time]]","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Mila-Oiva":{"title":"Mila Oiva","content":"\n## About\nMila Oiva is a cultural historian enthusiastic about identifying patterns of transnational circulation of knowledge and ideas in a long temporal perspective. Detecting the gaps, overflows, as well as geographical and social differences between the circulating information and perceptions reveal global interconnectednesses and differences. Mila explores circulation of knowledge through case studies focusing on circulation of pseudohistorical contents in Russian language internet in the 2000s and circulation of ideas and footage in Soviet newsreels after World War II. In her research, she analyses text, audio-visual and metadata using computational text, image and network analysis methods in interdisciplinary collaboration with other scholars.\n\n### Background\nMila received her PhD in Cultural History in 2017 from the University of Turku, Finland. She was a visiting Fulbright scholar at the Institute of Slavic, East European, and Eurasian Studies (ISEEES) at UC Berkeley in 2014-2015, and participated in the Culture Analytics long program at the Institute for Pure and Applied Mathematics (IPAM) at UCLA in spring 2016.\n\n### Field\n\nDigital #History with a focus on the #soviet era. #CulturalAnalytics \n\n### Interests/projects/skills\n- Soviet news and newsreels\n- Cold War era cultural diplomacy\n- Polish marketing practices in the Soviet Union\n- Interdisciplinary transfer of knowledge in Digital Humanities\n\n## Links\nhttps://cudan.tlu.ee/team/mila/","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Nanne-van-Noord":{"title":"Nanne van Noord","content":"\n\n## About\nAssistant Professor in the [Multimedia Analytics](https://multix.io/) lab of the University of Amsterdam. I‚Äôm interested in improving Multimedia Analysis with and for Visual Culture.\n\nBefore this I was a Postdoc in [CREATE](https://www.create.humanities.uva.nl/) working to improve the Computer Vision capabilties of the [CLARIAH](https://clariah.nl/) infrastructure. This followed my first Postdoc position at the [ISIS group](https://ivi.fnwi.uva.nl/isis/) in the [SEMIA](https://sensorymovingimagearchive.humanities.uva.nl/)project. In SEMIA I worked on techniques for exploring AV archives based on perceptual and sensory features.\n\n### Background\nI wrote my PhD thesis at Tilburg University on representation learning for artistic style, the digital version of which can be found [here](https://nanne.github.io/papers/thesis.pdf).\n\n![Nanne's PhD](https://nanne.github.io/assets/cover.png)\n\n\n### Field\n#MachineVision #ComputerVision #CulturalAnalytics\n\n### Interests/projects/skills\n- Computer vision \n- Cultural analytics\n- Visual representations of style\n- Interdisciplinary computational methods\n\n## Links\nhttps://nanne.github.io/about/","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Participants":{"title":"Participants","content":"---\nParticipants of the  [[public_notes/Sculpting Time with Computers/Sculpting Time With Computers|Sculpting Time With Computers]] workshop at KCL, 6-7 July, 2023.\n\n- [[public_notes/Sculpting Time with Computers/Andrea Farina|Andrea Farina]]\n- [[public_notes/Sculpting Time with Computers/Bel√©n Vidal|Bel√©n Vidal]]\n- [[public_notes/Sculpting Time with Computers/Carlo Bretti|Carlo Bretti]]\n- [[public_notes/Sculpting Time with Computers/Isadora Campregher|Isadora Campregher]]\n- [[public_notes/Sculpting Time with Computers/Jake Berger|Jake Berger]]\n- [[public_notes/Sculpting Time with Computers/Joel McKim|Joel McKim]]\n- [[public_notes/Sculpting Time with Computers/Mila Oiva|Mila Oiva]]\n- [[public_notes/Sculpting Time with Computers/Nanne van Noord|Nanne van Noord]]\n- [[public_notes/Sculpting Time with Computers/Pauline van Mourik Broekman|Pauline van Mourik Broekman]]\n- [[public_notes/Sculpting Time with Computers/Stephen McConnachie|Stephen McConnachie]]\n- [[public_notes/Sculpting Time with Computers/Tom Brown|Tom Brown]]\n\nAll information was pulled from publicly available sources online. Please contact me if you want something added, changed, or removed.","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Pauline-van-Mourik-Broekman":{"title":"Pauline van Mourik Broekman","content":"\n\n## About\nArtist and lecturer at the UAL's Creative Computing Institute. Co-founder of [Mute magazine](https://www.metamute.org/)that covered cyberculture, artistic practice, left-wing politics, urban regeneration, biopolitics, direct democracy, net art, the commons, horizontality and UK arts.\n\n### Background\nPhD at the Royal College of Art. Available from: https://researchonline.rca.ac.uk/5281/\n\n### Field\n\n#art #design #journalism\n\n### Interests/projects/skills\n- Art, design, and practice-based research\n- Tacit and embodied knowledge\n- Open education and cultural studies\n- Soviet film, twentieth century collaborative art practices, digital culture and economy\n\n## Links\nhttps://uk.linkedin.com/in/pauline-van-mourik-broekman-4a85b72\nhttp://www.techne.ac.uk/for-students/techne-students/techne-alumni-list/techne-students-2015/pauline-van-mourik-broekman","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Reverse-compression-as-motion-estimation":{"title":"Reverse compression as motion estimation","content":"---\n\nI wrote this for my PhD. I haven't found the actual experiment files, but the method can be recreated from the formulas if there's any interest in that. I can probably refine it a little bit if I implement it again.\n\n![[assets/Visual Energy_v0.3.pdf]]","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Sculpting-Time-With-Computers":{"title":"Sculpting Time with Computers","content":"---\n\n# Interdisciplinary Approaches to Computational Moving Images\n\nThis workshop brought together a group of researchers in the fields of digital and computational humanities, computer vision, film, digital preservation and archives, cultural history, and creative computing, to explore together emerging computational approaches to the study of time in moving images.\n\nIn this small net of notes, I document and summarise our workshop activities (and thanks to [[public_notes/Sculpting Time with Computers/Pauline van Mourik Broekman|Pauline]] for sharing her comprehensive notes which helped me put everything together in one place). Follow the links below for organic navigation, or find here [[public_notes/Sculpting Time with Computers/List of all notes|a list of all notes]]. \n\nThe work for this workshop was split over two days:\n\n-  On [[public_notes/Sculpting Time with Computers/day one|day one]] we explored the modelling of moving images as computational artefacts, and discussed the opportunities and challenges of computational approaches to large collections of moving images. \n\n- Based on these discussions, on [[public_notes/Sculpting Time with Computers/day two|day two]] we tested some of our ideas in practice using King's CREATE HPC cluster, kindly supported by [James Graham](https://www.kcl.ac.uk/people/james-graham) and [Matt Penn](https://www.kcl.ac.uk/people/matt-penn) from the [e-Research team](https://www.kcl.ac.uk/research/facilities/e-research)at KCL.\n\n\n![[assets/images/20230706_122721_r.jpg]]\n\n[[public_notes/Sculpting Time with Computers/Participants|Participants]] include researchers from leading laboratories in Europe, including the Cultural Data Analytics Open Lab ([CUDAN](https://cudan.tlu.ee/)) at Tallinn University and the Cultural Analytics Lab ([CANAL](http://canal-lab.uva.nl/)) at the University of Amsterdam, as well as archives and digital preservation experts from public UK institutions such as the [BBC](https://www.bbc.co.uk/archive/) and the [BFI](https://www2.bfi.org.uk/explore-film-tv/bfi-national-archive/about-bfi-national-archive/archive-teams/data-team). The workshop was hosted by the [Computational Humanities Research Group](https://www.kcl.ac.uk/research/computational-humanities-research-group) in the [Department of Digital Humanities](https://www.kcl.ac.uk/ddh) at King‚Äôs College London. And a big thanks to [King's Institute for Artificial Intelligence](https://www.kcl.ac.uk/ai) who kindly provided us with welcome packs, including totes, and notebooks.\n\n\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Stephen-McConnachie":{"title":"Stephen McConnachie","content":"\n\n## About\nStephen has worked at the BFI since 2009. In his current role he leads the Data and Digital Preservation department, with strategic and operational responsibility for the BFI National Archive‚Äôs data and digital preservation policies, standards, practices and infrastructure.\n\n### Background\nHe came to the BFI from the television archive ITN Source, where he led the teams of cataloguers, researchers and content editors in the transition from analogue to digital, file-based workflows.\n\n### Field\n#Archives #DigitalPreservation\n\n### Interests/projects/skills\nStephen's team defines and implements documentation and data standards for the BFI‚Äôs collections, creates the BFI Filmography (the national database of British feature films), manages the collections systems (including the [Collections Information Database](http://collections-search.bfi.org.uk/web) and the Digital Preservation Infrastructure), and delivers data and media to BFI platforms and projects, including BFI Player and the Mediatheque in BFI Southbank.\n\n## Links\nhttps://www2.bfi.org.uk/people/stephen-mcconnachie","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/Tom-Brown":{"title":"Tom Brown","content":"\n## About\nTom¬†Brown is Senior Lecturer in Film Studies. He is the author of two monographs: _Spectacle in ‚ÄúClassical‚Äù Cinemas: Musicality and Historicity in the 1930s_ (2016) and _Breaking the Fourth Wall: Direct Address in the Cinema_ (2012). Brown is the co-editor of _The Biopic in Contemporary Film Culture_ (2014), _Film Moments: Criticism, History, Theory_ (2010) and _Film and Television After DVD_ (2008), and the author of numerous other articles and chapters.\n\n### Background\nHe holds a PhD¬†in Film Studies and MA in Film and Television Studies from the University of Warwick and a BA in Film Studies and French from the University of Kent. Prior to working at KCL, he was a Lecturer in Film at the University of Reading.\n\n### Field\n#FilmStudies\n\n### Interests/projects/skills\nFilm analysis and film theory, with a focus on:\n- Cinematic ‚Äòclassicism‚Äô, particularly in Hollywood\n- The representation of history on film\n- French cinema, especially of the 1930s\n- Cinematic spectacle and film genres (esp. the musical)\n- Performance\n\n## Links\nhttps://www.kcl.ac.uk/people/tom-brown ","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/cinematic-time":{"title":"cinematic time","content":"---\n\nThis strand on table 3 focused on cinematic time at the most local level: from shot to shot and even frame to frame; a kind of computational poetics of moving images, which is probably the closest to my own work. We discussed more or less established methods to calculate cutting rates, such as average shot length (see for example [cinemetrics](https://cinemetrics.uchicago.edu/index.php) and this [shorter piece by Stephen Follows](https://stephenfollows.com/many-shots-average-movie/)), and their automation using techniques such as [shot boundary detection](https://en.wikipedia.org/wiki/Shot_transition_detection).\n\nOne of the ideas that emerged from these discussions is the subjective experience of film at a more perceptual (phenomenological?) level, and the language we use to describe that kind of experience, e.g. fast or slow. [[public_notes/Sculpting Time with Computers/Stephen McConnachie|Stephen]] elaborated on this and got us to ask if there might be a correlation between fast subjects and fast cutting rates, or between editing styles and the perception of time on screen. \n\nWhat would it take to engineer a fast-slow signal from sequences of frames?\nHow slow is [slow cinema](https://en.wikipedia.org/wiki/Slow_cinema)?\n\n![[assets/images/20230706_152240.jpg]]\n\nI mentioned the method we used to calculate the \"energy\" section of the [Made by Machine BBC programme in 2018](https://www.bbc.co.uk/programmes/b0bhwk3p), which basically consists on reverse engineering compression. I found [[public_notes/Sculpting Time with Computers/Reverse compression as motion estimation| some of my writing on the idea]] and some experiments I ran for my PhD. And [[public_notes/Sculpting Time with Computers/Carlo Bretti|Carlo Bretti]] ran an implementation of shot scale detection during [[public_notes/Sculpting Time with Computers/day two|day two]] of the workshop.\n\n\u003ciframe title=\"4. MbM: Motion Dynamics\" height=\"240\" width=\"426\" src=\"https://player.vimeo.com/video/429123481?app_id=122963\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.775 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/collections":{"title":"collections","content":"---\n\nIn this cluster are [[public_notes/Sculpting Time with Computers/Jake Berger|Jake Berger]] and [[public_notes/Sculpting Time with Computers/Stephen McConnachie|Stephen McConnachie]], who brought to the group their experience and knowledge working with archives and large collections of moving images, from an institutional perspective in the UK. Joined by [[public_notes/Sculpting Time with Computers/Mila Oiva|Mila Oiva]], from a research perspective analysing large collections of Soviet newsreels.\n\nThese are some of the challenges they highlighted for us in terms of encoding cinematic time for computational analysis: \n\n- Encoding performative aspects of carrier technology, for production and exhibition. For example common rules of thumb to estimate the frame rate of a projectionist cranking a film reel by hand in early films ([[public_notes/Sculpting Time with Computers/Stephen McConnachie|Stephen]]) or high frame rate such as in some of Ang Lee's films.\n- What's an acceptable levels of artifice: e.g. Peter Jackson's [frame interpolation and colorisation techniques](https://en.wikipedia.org/wiki/They_Shall_Not_Grow_Old) which was very controversial in the archiving community.\n- Encoding relational historical categories, such as \"Victorian\" or \"post-war\", and non-linear categories, such as cyclical time \"weekly\" ([[public_notes/Sculpting Time with Computers/Mila Oiva|Mila]]). This came up again on the discussions on [[public_notes/Sculpting Time with Computers/subjective time|subjective time]] during the ideation and prototyping sessions.\n- [[public_notes/Sculpting Time with Computers/Jake Berger|Jake]] also made the point of defining use cases for this research, keeping in mind the users and uses of archives, as well as the wider purpose of these public collections. He argued that some of the organisations best positioned to use archives data would be the least likely to be trusted with this data (eg. [FAANG's](https://en.wikipedia.org/wiki/Big_Tech)).","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/computational":{"title":"computational","content":"---\n\nIn the computational cluster we had [[public_notes/Sculpting Time with Computers/Nanne van Noord|Nanne van Noord]], [[public_notes/Sculpting Time with Computers/Carlo Bretti|Carlo Bretti]], and [[public_notes/Sculpting Time with Computers/Andrea Farina|Andrea Farina]]. They shared with the workshop the challenges of analysing moving images using units at different scales, e.g. pixels, shots, scenes, films, archives.See for example this table from my article on [[public_notes/Creanalytics|Creanalytics]]:\n\n|Artefact|Processing level|Example|\n|---|---|---|\n|Cinema|Social ‚Äì aggregate|Popular Hollywood cinema|\n|Film|Human|Jurassic Park ([1993](https://journals.sagepub.com/doi/10.1177/13548565231174592#bibr63-13548565231174592))|\n|Clip|Human/computer|Raptors in the Kitchen Scene ([https://youtu.be/dnRxQ3dcaQk](https://youtu.be/dnRxQ3dcaQk))|\n|Shot|Human-computer|130 frames (5.421 s)|\n|Frame|Computer/human|Individual frame (512 √ó 340 pixels)|\n|Pixels|Numeric ‚Äì disaggregate|Vector ([176800x1]); Tensor ([16, 3, 340, 512])|\n\n\nThis cluster also highlighted the difficulties of encoding weak conventions or tacit knowledge, which echoes some of the concerns of the [[public_notes/Sculpting Time with Computers/collections|collections]] cluster and the work of [[public_notes/Sculpting Time with Computers/Pauline van Mourik Broekman|Pauline]]. Later, during the ideation sessions, we came back for example to the idea of detecting \"scenes\" or \"blocs\" which are defined by semantic convention and defined [[public_notes/Sculpting Time with Computers/Annotation guidelines|annotation guidelines]] for this kind of unit.","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/day-one":{"title":"day one","content":"---\n\n## 6 July\n\n### Introduction session\nAfter informal conversations during coffee, we got started with a general introduction, where I gave an overview of the workshop, including its format, aims and, a provisional definition of cinematic time.\n\nAs I was organising this workshop, there were two references on the back of my mind:\n\n- Mary Ann Doane's excellent book [_The Emergence of Cinematic Time_](https://www.hup.harvard.edu/catalog.php?isbn=9780674007840), in which she makes a case for cinema transforming the very idea of time, and specifically, \"modern ideas about continuity and discontinuity, archivability, contingency and determinism, and temporal irreversibility.\" \n\n- Andrei Tarkovsky's [_Sculpting Time_](https://utpress.utexas.edu/9780292776241/), where he argues for an aesthetics of rhythm: \"The dominant, all-powerful factor of the film image is rhythm, expressing the course of time within the frame.\"\n\nI was glad to see that some of these themes emerged during discussion, and I should have made them clearer to participants, so I note them here to make them retrospectively available. \n\nTo kick things off, we split in four disciplinary clusters: [[public_notes/Sculpting Time with Computers/film|film]], [[public_notes/Sculpting Time with Computers/computational|computational]], [[public_notes/Sculpting Time with Computers/collections|collections]], and [[public_notes/Sculpting Time with Computers/design|design]].  The idea was to allow a disciplinary perspective to come through in this first round of discussions, to later reorganise into inter-disciplinary groups.\n\n(Thanks to [[public_notes/Sculpting Time with Computers/Joel McKim|Joel]] for the action shot below).\n\n![[assets/images/F1AeTakXsAcmmef.jpeg]] \n\n\n### Ideation session one: exploration\nHaving laid out some of the challenges and opportunities from our cluster perspective, we reorganised into mixed-perspective groups and got to work on a [[public_notes/Sculpting Time with Computers/long list of ideas|long list of ideas]]. This session was organised around the principle of fast iteration: collect as many ideas as possible, no matter how far-fetched, to quickly build a large pool of possibilities to work with.\n\n### Ideation session two: refinement\nFrom the long list, three ideas organically emerged as the ones participants were most interested in, so we organised these by types of time, one for each table: [[public_notes/Sculpting Time with Computers/historical time|historical time]], [[public_notes/Sculpting Time with Computers/subjective time|subjective time]], [[public_notes/Sculpting Time with Computers/cinematic time|cinematic time]]. We remixed again into different working groups, and went on to refine these three ideas by mapping some of the possible inputs and outputs relevant to each kind of time.\n\nNext, see how we developed this [[public_notes/Sculpting Time with Computers/day two|the next day]]!\n\n\n\n\n\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/day-two":{"title":"day two","content":"---\n\n## 7 July\n\n### Fast prototyping session\nOn day two, we gathered again to decide what ideas we could put to the test and fast-prototype. We examined two datasets: [[public_notes/Sculpting Time with Computers/Mila Oiva|Mila's]] soviet newsreels and this publicly available sample from [BBC Archives](https://archive-downloader.bbcrewind.co.uk/). We decided to focus on the newsreels, which have richer metadata. The idea was to test two methods that map to our discussions of [[public_notes/Sculpting Time with Computers/historical time|historical time]] and meaningful objects, and [[public_notes/Sculpting Time with Computers/cinematic time|cinematic time]] and fast/slow editing. We tested on the newsreels:\n\n- [Open clip](https://github.com/mlfoundations/open_clip) --\u003e an open source implementation of CLIP pretrained on the LAION 2B dataset. The core idea behind CLIP is to have text and images in a shared representational space that allows calculations of proximity between them. This can then be used for analysis and retrieval.\n\n![[assets/images/open_clip.png]]\n\n\n- [Cinescale](https://cinescale.github.io/) --\u003e a shot scale classification model trained on a set of manually annotated frames from 124 films. The goal is to classify different shots by their scale: close-up, medium shot, long shot, etc. In [[public_notes/Creanalytics|this paper]] I tried shot classification without annotations using face detection, but that approach is of course limited to faces, whereas the Cinescale library can in principle pick up close-ups of objects for example.\n\n![](https://cinescale.github.io/img/shottype.jpg)\n\n\nWe know these techniques have limitations, as explored during [[public_notes/Sculpting Time with Computers/day one|day one]] of the workshop. But we also think that, combined, these methods complement each other and can be useful for the analysis of temporal dynamics in large collections of moving images, for example as a way to detect editing patterns and co-relate them to themes and known historical time frames.\n\nTesting this combination against the newsreel data set is helpful because thanks to [[public_notes/Sculpting Time with Computers/Mila Oiva|Mila's]] sustained work on these reels, we know something about their local as well as global structures, so we can apply the methods to the collection of images and use the collection of images as a way to test the the methods. This approach reminded me of the notion of \"[predicting the past](http://www.digitalhumanities.org/dhq/vol/12/2/000377/000377.html)\" by the wonderful [Tobias Blanke](https://tobias-blanke.net/). \n\nDuring the [[public_notes/Sculpting Time with Computers/High-dimensional cinema|panel on high-dimensional cinema]], I referred to this back and forth between modalities of inquiry as building \"aesthetically sensitive systems\", and in our discussion the day after [[public_notes/Sculpting Time with Computers/Nanne van Noord|Nanne]] suggested \"aesthetically sensitive retrieval\" as an applied version of this idea. We wrapped up the day with some [[public_notes/Sculpting Time with Computers/ideas and next steps|ideas and next steps]] along these lines.\n\n[[public_notes/Sculpting Time with Computers/Mila Oiva|Mila]] and [[public_notes/Sculpting Time with Computers/Andrea Farina|Andrea]] also sampled the news reels and produced an [[public_notes/Sculpting Time with Computers/Annotation guidelines|annotation guidelines]] document for possible future human annotation, and that directly informs editing style detection and open CLIP queries. \n\n![[assets/images/20230707_170101.jpg]]\n\nThanks to [James Graham](https://www.kcl.ac.uk/people/james-graham) and [Matt Penn](https://www.kcl.ac.uk/people/matt-penn) from the e-research team at KCL, who helped us setup ad-hoc VMs to  run these tests in the CREATE HPC cluster from the Digital Humanities Computer Lab (pictured above).\n\n\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/design":{"title":"design","content":"---\n\nThe design cluster included [[public_notes/Sculpting Time with Computers/Pauline van Mourik Broekman|Pauline van Mourik Broekman]], [[public_notes/Sculpting Time with Computers/Joel McKim|Joel McKim]], and myself. We discussed some of the interfaces used to access and manipulate time in audiovisual media, for example the ubiquitous timeline in editing software. We discussed how interfaces structure the perception of viewing time, for instance in the default \"enforced flow\" of streaming platforms in which users have to explicitly tell the system to stop. Or the programmed deskilling of users through the constant reconfiguration of systems and interfaces.\n\nEchoing the [[public_notes/Sculpting Time with Computers/film|film]] and [[public_notes/Sculpting Time with Computers/collections|collections]] clusters, we also discussed some of the embodied, performative, and economic aspects in the production of films, specifically pre and post production practices. Pauline referenced Wendy Apple's documentary [The Cutting Edge](https://youtu.be/z-uJOyT_7i4) (not the [1992 movie](https://youtu.be/0jvRfLOVfeY)). Joel also mentioned how technical challenges in moving around data in space also shape the perceived temporality of media, he gave [Sohonet](https://www.sohonet.com/sohonet/about/) as an example.\n\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/film":{"title":"film","content":"---\n\nIn the film cluster we had [[public_notes/Sculpting Time with Computers/Bel√©n Vidal|Bel√©n Vidal]], [[public_notes/Sculpting Time with Computers/Tom Brown|Tom Brown]], and [[public_notes/Sculpting Time with Computers/Isadora Campregher|Isadora Campregher]].\n\nThey brought our attention issues of embodied spectatorship, including the difficulties of encoding  [[public_notes/Sculpting Time with Computers/high-level reasoning about time|high-level reasoning about time]] and [[public_notes/Sculpting Time with Computers/subjective time|subjective time]], which are needed to understand complex representations such as age in film. These levels were seen as mostly inaccessible to existing computational methods, and easily mobilised by human viewers.\n\nIt was interesting to see some of these limitations having correspondences in the [[public_notes/Sculpting Time with Computers/computational|computational]] cluster discussion about units of analysis.","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/high-level-reasoning-about-time":{"title":"high-level reasoning about time","content":"---\n\nThe [[public_notes/Sculpting Time with Computers/film|film]] cluster contributed by outlining the diversity of approaches and levels of analysis about time in moving images. Some of these include recognising and manipulating several representations of time in a global working space and using long-term memory, namely: \n\n- [[public_notes/Sculpting Time with Computers/cinematic time|motion time]]\n- character time \n- narrative time\n- film time\n- lived time \n- [[public_notes/Sculpting Time with Computers/historical time|historical time]]\n\nCharacter time and lived time, for example, are closely linked to the articulation of  [[public_notes/Sculpting Time with Computers/subjective time|subjective time]], also discussed during the ideation sessions.\n\nComputational models were recognised to be very limited in their capacity to simulate or perform this kind of reasoning. However, during the workshop we considered the opportunities of some of these methods to contribute to our understanding of some of these higher-level representations.","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/historical-time":{"title":"historical time","content":"---\n\nThis table was concerned with epochs or periods of time commonly used to describe, classify and retrieve moving images, using terms such as \"Victorian\", \"post-war\", \"pre-modern\", etc. [[public_notes/Sculpting Time with Computers/Jake Berger|Jake]]  and [[public_notes/Sculpting Time with Computers/Stephen McConnachie|Stephen]] mentioned this is often how archives are searched by users and for commercial licencing: \"I need a shot of a street in a rainy night in Victorian London\".  They agreed an \"epoch\" or \"era\" detector would be very helpful for archive users.\n\nWhat would it take to design an \"era detector\"?\n\nWe discussed possible approaches to infer this kind of _represented historical time_, including using detected objects as proxies for time periods, e.g. mobile phones, top hats, pylons, etc. This was provisionally called the _meaningful objects approach_, and the idea was to detect the first occurrence of these objects in large audiovisual archives. [[public_notes/Sculpting Time with Computers/Nanne van Noord|Nanne]]  noted that while this is possible, object detection is usually trained on contemporary objects, so there is an additional layer of modification needed.\n\n![[assets/images/20230706_152743.jpg]]\n\nWe tried for example to query \"Victorian\" in the [frozen in time video search](https://meru.robots.ox.ac.uk/frozen-in-time/) implementation, which outputs mostly buildings. One of the most revealing (and funny) moments was when we queried for \"Victorian person\":\n\n![[assets/images/victorian_person.png]]\n\nStill, the intuition is that something about depicted eras can be captured by this type of system stayed with us and was put to the test during [[public_notes/Sculpting Time with Computers/day two|day two]].\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/ideas-and-next-steps":{"title":"ideas and next steps","content":"---\n\nThree main ideas were discussed to follow up on the work from this workshop, more or less in order of immediacy and scale:\n\n- Submit a joint paper to the [CUDAN Cultural Analytics Conference](https://cudan.tlu.ee/conference/) in Tallinn, in December. [[public_notes/Sculpting Time with Computers/Mila Oiva|Mila]], [[public_notes/Sculpting Time with Computers/Nanne van Noord|Nanne]], and myself, are preparing an abstract based on the idea of \"aesthetically sensitive methods\" and the promising findings of the tests we ran on the collection of soviet newsreels during the second day. \n- Find ways to formalise the research network to strengthen our collaboration. The idea is to hold these meetings and workshops more regularly, allowing us to block some time for joint discussion and experimentation.\n- Explore if there is an appetite to develop a larger project proposal in partnership with one of the [[public_notes/Sculpting Time with Computers/collections|collections]].\n\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/long-list-of-ideas":{"title":"long list of ideas","content":"---\n\nWe split into three working groups mixing from our initial disciplinary clusters. The goal was to come up with as many ideas as possible for a system/tool/method that we thought would be valuable for investigating cinematic time. Pictures of the long lists below.\n\n## Long list table 1\n![[assets/images/20230706_122712.jpg]]\n\n## Long list table 2\n![[assets/images/20230706_122736.jpg]]\n\n## Long list table 3\n![[assets/images/20230706_123532.jpg]]\n","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Sculpting-Time-with-Computers/subjective-time":{"title":"subjective time","content":"---\n\nTime not only as duration, but as perceived through subjective experience, by audiences and by characters on screen. [[public_notes/Sculpting Time with Computers/Bel√©n Vidal|Bel√©n]] pointed our attention to the relation between time as duration, e.g. a performer's age in years, and subjective time, e.g. a character's age on screen. This was later discussed as one strand of encoding time using computational methods in the first and second ideation sessions, and alongside [[public_notes/Sculpting Time with Computers/historical time|historical time]] and [[public_notes/Sculpting Time with Computers/cinematic time|cinematic time]].\n\nFor the refinement session, we considered possible methods to analyse age and ageing on screen, including character age, performer age, and the gap in between. Here is how this table looked during this refinement stage:\n\n![[assets/images/20230706_151508.jpg]]\n\nAs a starting point, we considered what it would take to get the difference between performer age and performed age, split by gender. Performer age as a duration can be calculated by identifying the date of birth of performer, e.g. Sean Connery (1930), and their approximate age at a given performance, e.g. _Diamonds are Forever (1971)_, which means he was ~41 at the time of filming.\n\nPerformed age is much more difficult. We discussed possible methods for performed age detection, including extracted features from faces, speech and even gestures ‚Äïall of which are we thought tended to be noisy/inaccurate/biased. We also considered text sources such as synopses, where main characters are sometimes given age descriptors such as \"ten-year-old Finnegan\" or relational such as \"her mother Estella\". Other sources include scripts, which can be more accurate but as [[public_notes/Sculpting Time with Computers/Jake Berger|Jake]] reminded us are also less publicly available, and in some cases also the original literary sources of film adaptations (I think it was [[public_notes/Sculpting Time with Computers/Andrea Farina|Andrea]] who mentioned this).\n\nIn terms of gender, [[public_notes/Sculpting Time with Computers/Mila Oiva|Mila]] and [[public_notes/Sculpting Time with Computers/Isadora Campregher|Isadora]] referred to their experience with gender detection by name, and [[public_notes/Sculpting Time with Computers/Stephen McConnachie|Stephen]] was in touch later to contribute the [INA speech segmenter with gender identification](https://github.com/ina-foss/inaSpeechSegmenter), and his project on [gender inference by first name](https://www.nesta.org.uk/blog/women-in-film-what-does-the-data-say/).","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/The-Digital-Pastoral":{"title":"The Digital Pastoral","content":"---\n\nI created the image below using #stablediffusion (before it was cool and every media scholar started doing it) and used it for my very short presentation at [[Transmediale 2023 | Transmediale 2023]].\n\n\n![[assets/minor_tech_DCH.jpg]]\n\nI can't recall the exact prompt, but I do remember I referenced the illustrations on [Jakub R√≥≈ºalski](https://jrozalski.com/) whom I discovered through [Scythe](\u003chttps://en.wikipedia.org/wiki/Scythe_(board_game)\u003e).\n\nAnd here's [a link to the newspaper](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf)!","lastmodified":"2023-08-10T10:28:46.336415271Z","tags":null},"/public_notes/Tools/Mermaid-diagrams":{"title":"Mermaid diagrams","content":"\n\u003eJavaScript based diagramming and charting tool that renders Markdown-inspired text definitions to create and modify diagrams dynamically.\n\nIt has a [live editor](https://mermaid.live/) for non-programmers, and has [some tutorials](https://mermaid.js.org/config/Tutorials.html). Maybe relevant for students. There's a Jupyter integration too. Interesting for #criticaltechnicalpractice \nMaybe I can use this to render an maintain a web version of the [[private/Projects/Large/VANGE]] diagram in my office.\n\n![Mermaid](https://mermaid.js.org/header.png)\n\n","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/Tools/Motion-Canvas":{"title":"Motion Canvas","content":"---\n\n\u003eVisualize Complex Ideas Programmatically.\n\u003eIt's a specialized tool designed to create informative vector animations and synchronize them with voice-overs. It's not meant to be a replacement for traditional video editing software.\n\nProcedural animation tool, that can be integrated with a web-editor to sync sound and produce animated videos. Great for education, documentation, and reflection in #criticaltechnicalpractice Reminds me a bit of Flash action script, but #FOSS and nicer for rapid development and to comunicate abstract ideas precisely. Think of [Freya Holm√©r](https://www.youtube.com/@Acegikmo) videos or [Ms Coffee Beans](https://www.youtube.com/@AICoffeeBreak).\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/H5GETOP7ivs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\n\n","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/Tools/Movie-Barcodes":{"title":"Movie Barcodes","content":"---\n\nThere are a few tools online to generate [\"movie barcodes\"](https://thefilmstage.com/movie-barcode-an-entire-feature-film-in-one-image/) like this one, from [Made by Machine](https://movingpixel.net/project/mbm/):\n\n![[assets/made_by_machine_whole_barcode.jpg]]\n\n\nThe process variously involves sampling frames from a video file, and stretching them as lines in chronological order to visualise the colour-time dynamics of the underlying moving images. The idea has been around for a while, and there are now on-demand printing services that sell you a poster of your favourite film bar code.\n\nThis involves a relatively simple processing and manipulation of [[public_notes/Video as data|Video as data]], with no machine learning involved. Yet, it tells us something about the films, I think it can be used as a pre or post processing technique in combination with other methods.\n\n- [Movie Barcodes](https://timbennett.github.io/movie-barcodes/) --\u003e backend: [FFMPEG](https://www.ffmpeg.org/) + [PIL](https://en.wikipedia.org/wiki/Python_Imaging_Library)\n- [Movie Barcode](https://github.com/MarcBresson/movie-barcode/tree/main) --\u003e wrapped as a Python library\n- [movie-barcodes](https://github.com/andrewdcampbell/movie-barcodes) --\u003e command line application\n\nI've been trying these out for the book cover of my book, possibly warped as as a circle, similar to [this example](https://rlang.io/create-a-radial-movie-tv-barcode-using-polar-coordinates/) (in R). The polar transformation seems rather onerous to implement directly in python (for me anyway), but a similar effect can be achieved using the polar filter in [GIMP](https://www.gimp.org/), or the ImageMagick's [polar distortion transformation](https://imagemagick.org/Usage/distorts/#polar).","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/Tools/Streaming-Video-Model":{"title":"Streaming Video Model","content":"---\n\n[Interesting video model](https://arxiv.org/abs/2303.17228#) that accounts for frames and sequences in a unified model. \nAbstract:\n[](https://arxiv.org/search/cs?searchtype=author\u0026query=Zha%2C+Z)\n\n\u003e Video understanding tasks have traditionally been modeled by two separate architectures, specially tailored for two distinct tasks. Sequence-based video tasks, such as action recognition, use a video backbone to directly extract spatiotemporal features, while frame-based video tasks, such as multiple object tracking (MOT), rely on single fixed-image backbone to extract spatial features. In contrast, we propose to unify video understanding tasks into one novel streaming video architecture, referred to as Streaming Vision Transformer (S-ViT). S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks. Then the frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks. The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. We believe that the concept of streaming video model and the implementation of S-ViT are solid steps towards a unified deep learning architecture for video understanding. Code will be available at [this https URL](https://github.com/yuzhms/Streaming-Video-Model).\n\n","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/Tools/Videogrep":{"title":"Videogrep","content":"---\n\n\u003eVideogrep is a command line tool that searches through dialog in video files and makes [supercuts](https://vimeo.com/440746435) based on what it finds. It will recognize .srt or .vtt subtitle tracks, or transcriptions that can be generated with vosk, pocketsphinx, and other tools.\n\nSam posted an implementation of Videogrep that runs on Colab, which made me think of running this on a Jupyter Lab instance from our College's computer cluster, and then possibly using it for teaching. The college does not host its own notebook instance, as far as I know, but the cluster can serve a headless Jupyter that can be \"tunnelled in\" through ssh.\n\nHere's a supercut of [recent lecture](https://www.youtube.com/watch?v=b6ogLgWnpes) by  [Shannon Mattern](https://cinemastudies.sas.upenn.edu/people/shannon-mattern) at KCL called \"Modeling Doubt, Coding Humility: A Speculative Syllabus\"\n\n\u003ciframe title=\"Doubt ‚Äï with Shannon Mattern\" height=\"240\" width=\"426\" src=\"https://player.vimeo.com/video/830237949?h=5fb9301c4d\u0026amp;app_id=122963\" allowfullscreen=\"\" allow=\"fullscreen\" style=\"aspect-ratio: 1.775 / 1; width: 100%; height: 100%;\"\u003e\u003c/iframe\u003e\n\n\nInterestingly, not one single mention of \"code\" or \"coding\", nor of \"humility\". I recommend watching the whole talk, though.\n\nHere's the [repository](https://github.com/antiboredom/videogrep)for Videogrep. Here's the [Colab minimal example](https://t.co/QGKTLxOZ52).\nAnd here are other examples from Sam's repo:\n\n-   [silence extraction](https://github.com/antiboredom/videogrep/blob/master/examples/only_silence.py)\n-   [automatically creating supercuts](https://github.com/antiboredom/videogrep/blob/master/examples/auto_supercut.py)\n-   [creating supercuts based on youtube searches](https://github.com/antiboredom/videogrep/blob/master/examples/auto_youtube.py)\n-   [creating supercuts from specific parts of speech](https://github.com/antiboredom/videogrep/blob/master/examples/parts_of_speech.py)\n-   [creating supercuts from spacy pattern matching](https://github.com/antiboredom/videogrep/blob/master/examples/pattern_matcher.py)","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/Tools/Zeeschuimer":{"title":"Zeeschuimer","content":"---\n\n\u003e Zeeschuimer is a browser extension that monitors internet traffic while you are browsing a social media site, and collects data about the items you see in a platform's web interface for later systematic analysis. Its target audience is researchers who wish to systematically study content on social media platforms that resist conventional scraping or API-based data collection.\n\n\n![Zeeschuimer](https://github.com/digitalmethodsinitiative/zeeschuimer/blob/master/images/example_screenshot.png?raw=true)\n\nThis would definitely be of interest to some of my UX students as a way to integrate in their usability tests and cultural probes when analysing social media usage.\nRepository [here](https://github.com/digitalmethodsinitiative/zeeschuimer).\n\n\n\n\n\n\n","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/Transmediale-2023":{"title":"Transmediale 2023","content":"---\n\nI attended [Transmediale](https://2023.transmediale.de/en) and gave a very small presentation as part of the panel *Toward Minor Tech*.\n\n[![Towards a Minor Tech Newspaper](assets/images/minor_tech_news.jpeg)](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf)\n\nThis is the result of a workshop organised by by Geoff Cox and Christian Ulrik Andersen every year as part of [SHAPE Digital Citizenship](https://shape.au.dk/en/) \u0026 [Digital Aesthetics Research Center, Aarhus University](https://darc.au.dk/) and [Centre for the Study of the Networked Image, London South Bank University](https://www.centreforthestudyof.net/), this year in collaboration with [King‚Äôs College London](https://www.kcl.ac.uk/). \n\nThe workshop took place over several days in London and inlcuded public talks from Marloes de Valk on the [damaged earth catalog](https://damaged.bleu255.com/) and from Tung-Hui Hu on [digital lethargy](https://thephotographersgallery.org.uk/whats-on/talk-digital-lethargy).\nMy contribution to the publication was titled: *A minor critique of minor tech* and can be found as a PDF, along with all other contirbutions, by clicking [here](https://darc.au.dk/fileadmin/DARC/newspapers/toward-a-minor-tech-online-sm.pdf).\n\n![minor tech panel](assets/images/minor_tech_panel.jpeg)\n","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/Video-as-data":{"title":"Video as data in the US","content":"---\n\nThis looks interesting!\nSee twitter post [here](https://twitter.com/Kaiping_Chen/status/1660703434155106313).\n\n![[assets/images/Pasted image 20230523190034.png]]","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/Video-understanding-community":{"title":"Video-understanding community","content":"---\n\nSome interesting recent work on video understanding tasks and practitioners from computer science. \n\n[CMD Challenge](https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/challenge.html)\nFrom the [VGG group at Oxford](https://www.robots.ox.ac.uk/~vgg/). It uses a version of the [condensed movie dataset](https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/)challenge, based on the same Movieclip collection I describe in [[public_notes/Creanalytics|Creanalytics]]. See [their repo](https://github.com/m-bain/CondensedMovies-chall) for challenge details, and the description below:\n\n\u003e__Focus__: The focus of this challenge is on the long-range understanding of high-level narrative structure in movies.\n\n\u003e__Overview__: In the challenge, participants are invited to build a system to retrieve 2-3 minute video clips from movies using corresponding high-level natural language descriptions and a wide range of pre-computed visual features from several pre-trained expert models. Each 2-3 minute clip constitutes a key scene from a movie, each representing important parts in the storyline. Each clip is accompanied by a high-level semantic description which describes the storyline. This includes the motivations of the characters, actions, scenes, objects, interactions and relationships. Participants will use a new challenge version of the Condensed Movies Dataset (CMD) for both training and testing of their retrieval systems. \n\nThis group/challenge is related to the [[public_notes/Video as data|Video as data]] ICA workshop. And also to a broader push to integrate natural language and visual understandings. See for example the work of [Max Bain](https://www.maxbain.com/), [Lisa Anne Hendricks](https://lisaanne.github.io/) at Deepmind and [Andrew Brown](https://www.robots.ox.ac.uk/~abrown/), ex VGG now at Meta. See below:\n\nhttps://www.youtube.com/watch?v=GzIphByhXDc\n","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null},"/public_notes/small-large-LLMs":{"title":"small large LLMs","content":"---\nMy colleague [Mercedes Bunz](https://mercedesbunz.net/) made me aware of this allegedly [leaked document](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) from a Google engineer, in which they make the case for open-sourcing their models. TLDR; The argument is that owning and cultivating the ecosystem for innovation is more valuable than keeping the models fenced off.\n\nCaveats notwithstanding, say for instance that the diminishing value of training does not account for people's salaries in publicly-funded institutions, it is still an interesting read, especially the timeline narrating all the developments. Good for teaching, but also to make sense of the various recent moves in the field.\n\nSome of the models and mentioned:\n[LLaMA](https://arxiv.org/pdf/2303.16199.pdf) ‚Äï Meta\n[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)‚Äï Stanford\n[Alpaca LoRA](https://github.com/tloen/alpaca-lora)‚Äï Stanford + Eric Wang. See [paper here](https://arxiv.org/pdf/2106.09685.pdf).\n[A Chatbot interface for Alpaca](https://github.com/deep-diver/LLM-As-Chatbot)\n[Dolly 15k instructions dataset](https://huggingface.co/datasets/c-s-ale/dolly-15k-instruction-alpaca-format)\n\u003e `databricks-dolly-15k` is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language models to exhibit the magical interactivity of ChatGPT.\n\n[GPT4all](https://github.com/nomic-ai/gpt4all)‚Äï open stack pipeline based on JPT-J and LLaMA\n[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)\n\u003ean open-source Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT\n\nThis las one was developed by a student-led university consortium called [LMSYS Org](https://lmsys.org/about/)!\n\n","lastmodified":"2023-08-10T10:28:46.340415369Z","tags":null}}