---
title: "small large LLMs"
date: 05 May 2023
draft: false
tags:
- LLMs
- tools
---
---
My colleague [Mercedes Bunz](https://mercedesbunz.net/) made me aware of this allegedly [leaked document](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) from a Google engineer, in which they make the case for open-sourcing their models. TLDR; The argument is that owning and cultivating the ecosystem for innovation is more valuable than keeping the models fenced off.

Caveats notwithstanding, say for instance that the diminishing value of training does not account for people's salaries in publicly-funded institutions, it is still an interesting read, especially the timeline narrating all the developments. Good for teaching, but also to make sense of the various recent moves in the field.

Some of the models and mentioned:
[LLaMA](https://arxiv.org/pdf/2303.16199.pdf) ― Meta
[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)― Stanford
[Alpaca LoRA](https://github.com/tloen/alpaca-lora)― Stanford + Eric Wang. See [paper here](https://arxiv.org/pdf/2106.09685.pdf).
[A Chatbot interface for Alpaca](https://github.com/deep-diver/LLM-As-Chatbot)
[Dolly 15k instructions dataset](https://huggingface.co/datasets/c-s-ale/dolly-15k-instruction-alpaca-format)
> `databricks-dolly-15k` is a corpus of more than 15,000 records generated by thousands of Databricks employees to enable large language models to exhibit the magical interactivity of ChatGPT.

[GPT4all](https://github.com/nomic-ai/gpt4all)― open stack pipeline based on JPT-J and LLaMA
[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
>an open-source Chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT

This las one was developed by a student-led university consortium called [LMSYS Org](https://lmsys.org/about/)!

