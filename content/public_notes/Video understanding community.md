---
title: "Video-understanding community"
date: 03 Jul 2023
draft: false
tags:
- video
- machine vision
---
---

Some interesting recent work on video understanding tasks and practitioners from computer science. 

[CMD Challenge](https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/challenge.html)
From the [VGG group at Oxford](https://www.robots.ox.ac.uk/~vgg/). It uses a version of the [condensed movie dataset](https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/)challenge, based on the same Movieclip collection I describe in [[public_notes/Creanalytics|Creanalytics]]. See [their repo](https://github.com/m-bain/CondensedMovies-chall) for challenge details, and the description below:

>__Focus__: The focus of this challenge is on the long-range understanding of high-level narrative structure in movies.

>__Overview__: In the challenge, participants are invited to build a system to retrieve 2-3 minute video clips from movies using corresponding high-level natural language descriptions and a wide range of pre-computed visual features from several pre-trained expert models. Each 2-3 minute clip constitutes a key scene from a movie, each representing important parts in the storyline. Each clip is accompanied by a high-level semantic description which describes the storyline. This includes the motivations of the characters, actions, scenes, objects, interactions and relationships. Participants will use a new challenge version of the Condensed Movies Dataset (CMD) for both training and testing of their retrieval systems. 

This group/challenge is related to the [[public_notes/Video as data|Video as data]] ICA workshop. And also to a broader push to integrate natural language and visual understandings. See for example the work of [Max Bain](https://www.maxbain.com/), [Lisa Anne Hendricks](https://lisaanne.github.io/) at Deepmind and [Andrew Brown](https://www.robots.ox.ac.uk/~abrown/), ex VGG now at Meta. See below:

https://www.youtube.com/watch?v=GzIphByhXDc
